<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Aleksey Nozdryn-Plotnicki]]></title>
  <link href="http://alekseynp.com/atom.xml" rel="self"/>
  <link href="http://alekseynp.com/"/>
  <updated>2015-03-24T20:51:15-07:00</updated>
  <id>http://alekseynp.com/</id>
  <author>
    <name><![CDATA[Aleksey Nozdryn-Plotnicki]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Ontarion Sunshine List Pathologists Notebook]]></title>
    <link href="http://alekseynp.com/2015/03/24/gist-pathologists/"/>
    <updated>2015-03-24T13:00:00-07:00</updated>
    <id>http://alekseynp.com/2015/03/24/gist-pathologists</id>
    <content type="html"><![CDATA[<p>Here is an <a href="http://nbviewer.ipython.org/gist/alekseynp/81a249e5e971750039dc">nbviewer IPython notebook showing an analysis of pathologists in the Ontario Sunshine List</a>.</p>

<p>It uses data from the <a href="https://github.com/alekseynp/ontario_sunshine_list">Ontario Sunshine List Scraper</a> that I made public on GitHub recently.</p>

<p>It recreates the analysis that went in to my previous article about pathologists on the list: <a href="http://alekseynp.com/2013/04/19/20-25-raise-for-ontarios-pathologists-in-201-shows-sunshine-list/">20-25% Raise for Ontario’s Pathologists in 2012</a></p>

<p>I hope you&rsquo;ll find it useful.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ontario Sunshine List Open Scraper]]></title>
    <link href="http://alekseynp.com/2015/03/07/ontario-sunshine-list-scraper/"/>
    <updated>2015-03-07T08:30:00-08:00</updated>
    <id>http://alekseynp.com/2015/03/07/ontario-sunshine-list-scraper</id>
    <content type="html"><![CDATA[<p>Today I am announcing my open Ontario Sunshine List Scraper released under the MIT License.</p>

<p><strong>You can download the data directly <a href="https://github.com/alekseynp/ontario_sunshine_list/blob/master/output/data.csv">here</a>.</strong> I will likely move this later.</p>

<p>Anyone can go <a href="https://github.com/alekseynp/ontario_sunshine_list">here to the GitHub repo</a> and download python code that scrapes the <a href="http://www.fin.gov.on.ca/en/publications/salarydisclosure/pssd/">Ontario Public Sector Salary Disclosure</a> data into a machine-readable format.</p>

<p>Today&rsquo;s version of the code has two key limitations:</p>

<ul>
<li>Only the initial disclosure is scraped. Addenda are not scraped or processed.</li>
<li>2015 disclosure has not yet been published or included</li>
</ul>


<p>Please feel free to fork the repo and build ahead.</p>

<p>Anyone can create their own CSV like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import ontario_sunshine_list as osl
</span><span class='line'>
</span><span class='line'>col = osl.Collector()
</span><span class='line'>col.run('/home/aleksey/data/sunshine/')
</span><span class='line'>
</span><span class='line'>scr = osl.Scraper()
</span><span class='line'>df = scr.run('/home/aleksey/data/sunshine/')
</span><span class='line'>
</span><span class='line'>cle = osl.Cleaner()
</span><span class='line'>df = cle.run(df)
</span><span class='line'>
</span><span class='line'>df.to_csv('/home/aleksey/data.csv', encoding='utf-8')</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[\"Charter\" Schools Have Worse Immunization Rates in California]]></title>
    <link href="http://alekseynp.com/2015/02/24/california-charter-kindergartens/"/>
    <updated>2015-02-24T10:30:18-08:00</updated>
    <id>http://alekseynp.com/2015/02/24/california-charter-kindergartens</id>
    <content type="html"><![CDATA[<p>In California, if your Kindergarten has the word &ldquo;Charter&rdquo; in its name, that says a lot about its measles vaccination rate.</p>

<p><img src="http://alekseynp.com/images/Charter_Measels.png" alt="" /></p>

<ul>
<li>&ldquo;Charter&rdquo; kindergartens are almost twice as likely to have a measles vaccination rate below 92-94% for herd immunity.</li>
<li>&ldquo;Charter&rdquo; kindergartens have an average measles immunization rate of 83.8% compared to 91.8% in other schools.</li>
<li>Personal Beliefs Exemptions were 10.8% in &ldquo;Charter&rdquo; schools compared to 3% in all others.</li>
</ul>


<p>Credit: Pattern first identified via visual inspection of <a href="http://www.nytimes.com/interactive/2015/02/06/us/california-measles-vaccines-map.html?_r=0">Vaccination Rates for Every Kindergarten in California</a> from the <a href="http://www.nytimes.com/">New York Times</a>.</p>

<p>Data: Available on the <a href="https://chhs.data.ca.gov">California Health and Human Services Open Data Portal</a>. Specifically the School Immunizations in Kindergarten 2014-15 dataset, <a href="https://cdph.data.ca.gov/Diseases-and-Conditions/School-Immunizations-In-Kindergarten-2014-2015/4y8p-xn54">here</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Regressing My Gas Log]]></title>
    <link href="http://alekseynp.com/2014/11/22/regressing-my-gas-log/"/>
    <updated>2014-11-22T11:30:18-08:00</updated>
    <id>http://alekseynp.com/2014/11/22/regressing-my-gas-log</id>
    <content type="html"><![CDATA[<p>What mileage does your vehicle get? Well that depends. City or highway? Well what is your fuel efficiency in each? Today I geeked it up and did a regression on my gas log to determine just this. Yes, I keep a gas log. It pleases my inner scientist and it helps me watch for sudden changes in fuel efficiency.</p>

<p>Raw data, <strong>Fill Ups</strong>:
kms | litress | efficiency | est_mix_city | est_mix_highway
&mdash; | &mdash; | &mdash; | &mdash; | &mdash;
420.8 | 46.514 | 11.05370722 | 0.5 | 0.5
576.4 | 59.53 | 10.32789729 | 0 | 1
505.2 | 51.602 | 10.2141726 | 0 | 1
522 | 54.273 | 10.39712644 | 0 | 1
305.3 | 32.433 | 10.62332132 | 0 | 1
1111.4 | 108.578 | 10.73541625 | 0.2 | 0.8
508.1 | 53.112 | 10.45306042 | 0.1 | 0.9
496.4 | 52.4393 | 10.56392023 | 0.1 | 0.9
442.1 | 44.5392 | 10.07446279 | 0 | 1
393.4 | 43.239 | 10.9911032 | 0.1 | 0.9
429.2 | 45.345 | 10.56500466 | 0 | 1
476.7 | 58.217 | 12.21250262 | 0.85 | 0.15</p>

<p>In google docs it is a simple thing to run <code>=SLOPE(efficiency rows,est_mix_city rows)</code> and <code>=INTERCEPT(efficiency rows,est_mix_city rows)</code> giving me 10.381 and 1.967.</p>

<p>Simple interpretation is then:
- With a city mix of 0 and therefore 100% highway, I would get 10.381 + 1.967<em>0 = 10.381 L/100 km.
- With a city mis of 1 and therefore 100% city, I would get 10.381 + 1.967</em>1 = 12.348 L/100 km.</p>

<p>Before you make any snide comments, I drive a two-ton 20-year-old 4x4 diesel van. You&rsquo;ll notice mostly not in the city. It&rsquo;s good at some things, not so good at others.</p>

<p>Yes, there are probably better techniques to get an estimate that perhaps weight bigger fill ups higher, or recognize the small number of data points. But whatever, this was a great way to get to a first estimate.</p>

<p>(Aside: It&rsquo;s interesting that I am happy to use the term &ldquo;mileage&rdquo; to describe a statistics that I will measure in L/100km)</p>

<h2>#</h2>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Visualizing Confirmed Exoplanets]]></title>
    <link href="http://alekseynp.com/2014/05/16/visualizing-confirmed-exoplanets/"/>
    <updated>2014-05-16T12:30:18-07:00</updated>
    <id>http://alekseynp.com/2014/05/16/visualizing-confirmed-exoplanets</id>
    <content type="html"><![CDATA[<p>I’ve just published a post over at the NGRAIN blog, <a href="http://www.ngrain.com/visualize-confirmed-exoplanets/">Visualize confirmed exoplanets</a></p>

<p>I used NGRAIN&rsquo;s Constructor SDK to put together a simple visualization of the distribution of confirmed exoplanets relative to Earth.</p>

<p>By no means a feature-rich interactive visualization, it&rsquo;s simple, powerful, and interesting.</p>

<h3></h3>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gephi Layout Plugin: Random 3D Layout]]></title>
    <link href="http://alekseynp.com/2014/05/15/gephi-layout-plugin-random-3d-layout/"/>
    <updated>2014-05-15T11:02:45-07:00</updated>
    <id>http://alekseynp.com/2014/05/15/gephi-layout-plugin-random-3d-layout</id>
    <content type="html"><![CDATA[<p><img src="http://alekseynp.com/wp-content/uploads/2014/05/bigRandomCube-300x266.png" alt="bigRandomCube" />Yesterday, I released a simple plugin, <a href="https://marketplace.gephi.org/plugin/random-3d-layout/">Random 3D Layout t</a>o the  Gephi Marketplace <a href="https://marketplace.gephi.org/plugin/random-3d-layout/">here</a>. It&rsquo;s very simple, and I developed it as part of my work at <a href="http://www.ngrain.com">NGRAIN</a>.</p>

<p>For anyone working in 3D in Gephi, this will be a useful and simple initialization layout plugin. Works well with the <a href="https://marketplace.gephi.org/plugin/force-atlas-3d/">Force Atlas 3D Plugin</a>. Without it and using a 2D layout to initialize instead, the 3D results can sometimes be milky-way-shaped, not generally spherical as you would expect, and ultimately not taking full advantage of the third dimension.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Augmented Reality Demands 3D Data Visualization]]></title>
    <link href="http://alekseynp.com/2014/04/22/augmented-reality-demands-3d-data-visualization/"/>
    <updated>2014-04-22T08:13:15-07:00</updated>
    <id>http://alekseynp.com/2014/04/22/augmented-reality-demands-3d-data-visualization</id>
    <content type="html"><![CDATA[<p>I’ve just written a post over at the NGRAIN blog, <a href="http://www.ngrain.com/augmented-reality-demands-3d-data-visualization/">Augmented Reality demands 3D Data Visualization</a></p>

<blockquote>As Augmented Reality (AR) continues to gain strong momentum, it is clear that data and information now have a physical 3D context, both conceptually and at the point of display. In the past, we were content with excellent 2-dimensional solutions for 2-dimensional displays, free to ignore 3D data viz as fraught with peril and difficult to execute. At a minimum, we must now wrestle with the challenge of presenting 2D visualizations in a 3D world. And, if we limit ourselves to that, we will fail.</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Looking Beyond Two Dimensions]]></title>
    <link href="http://alekseynp.com/2014/04/09/on-looking-beyond-two-dimensions/"/>
    <updated>2014-04-09T09:30:20-07:00</updated>
    <id>http://alekseynp.com/2014/04/09/on-looking-beyond-two-dimensions</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve just written a post over at the NGRAIN blog, <a href="http://www.ngrain.com/on-looking-beyond-two-dimensions/">On looking beyond two dimensions</a>.</p>

<blockquote>If you don’t look at your data it can deceive you. That was true in two dimensions and is true above. If we live in a world of bivariate visualization we will miss important complex patterns because we are not looking for them. The third dimension must be considered when using visualization in analysis just as it must be rejected as a means to jazz up uninspiring pie charts.</blockquote>


<p><img src="http://www.ngrain.com/wp-content/uploads/2014/04/movie8.gif" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data Visualization Lead at NGRAIN]]></title>
    <link href="http://alekseynp.com/2014/04/08/data-visualization-lead-at-ngrain/"/>
    <updated>2014-04-08T14:09:24-07:00</updated>
    <id>http://alekseynp.com/2014/04/08/data-visualization-lead-at-ngrain</id>
    <content type="html"><![CDATA[<p>It&rsquo;s worth announcing that last month I took a position as Data Visualization Lead at <a href="http://www.ngrain.com/">NGRAIN</a> in Vancouver, Canada.</p>

<p>NGRAIN&rsquo;s vision is to see beyond reality, and to help people accelerate decisions by interacting with the world&rsquo;s data in 3D. To make that concrete, we are currently developing cutting edge Augmented Reality applications for the industrial enterprise. 3D data visualization is notoriously difficult to execute, but in Augmented Reality we will no longer have a choice but to confront it, as our reality is 3D. Bringing my expertise to this problem will be an exciting challenge and will require great care.</p>

<p>What you see here will continue to be my own views and not those of NGRAIN. I will, however, likely be publishing on NGRAIN&rsquo;s <a href="http://www.ngrain.com/blog/">blog</a> and cross-promoting here.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My Ontario Sunshine List Work in the CBC News]]></title>
    <link href="http://alekseynp.com/2014/04/01/my-ontario-sunshine-list-work-in-the-cbc-news/"/>
    <updated>2014-04-01T09:13:29-07:00</updated>
    <id>http://alekseynp.com/2014/04/01/my-ontario-sunshine-list-work-in-the-cbc-news</id>
    <content type="html"><![CDATA[<p>Just some shameless self-promotion. You can find my work cited in the CBC News from today.</p>

<p>Kazi Stastna writes <a href="http://www.cbc.ca/news/canada/sunshine-list-2014-ontario-s-list-drives-salaries-up-not-down-1.2592793">Sunshine List 2014: Ontario&rsquo;s list drives salaries up, not down</a> and includes:</p>

<blockquote>Ontario pathologists did just that and saw their Sunshine List salaries increase by 20 to 25 per cent between 2011 and 2012, compared with the 2.2 per cent average for the list as a whole, according to an [analysis](http://alekseynp.com/2013/04/19/20-25-raise-for-ontarios-pathologists-in-201-shows-sunshine-list/) done by data blogger Aleksey Nozdryn-Plotnicki.

&#8230;

Nozdryn-Plotnicki [found](http://alekseynp.com/2013/04/19/7-2-raise-for-1000-best-paid-ontario-public-sector-employees/) that this inflationary effect is greatest at the upper echelons of the Sunshine List, with the salaries of the 1,000 highest-paid workers rising 7.2 per cent between 2011 and 2012, compared with 2.2 per cent for the bottom half of the list.</blockquote>


<p>They are, of course, referring to my work:</p>

<ul>
<li><p><a href="http://alekseynp.com/2013/04/19/7-2-raise-for-1000-best-paid-ontario-public-sector-employees/">7.2% raise for 1,000 best paid Ontario public sector employees</a></p></li>
<li><p><a href="http://alekseynp.com/2013/04/19/20-25-raise-for-ontarios-pathologists-in-201-shows-sunshine-list/">20-25% raise for Ontario’s pathologists in 2012
</a></p></li>
</ul>


<p>For those outside of Canada, the CBC, the Canadian Broadcasting Corporation is a public entity and major player in the Canadian news media market. It&rsquo;s my personal number one source for Canadian news.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[I Tried to Sell Open Government Data]]></title>
    <link href="http://alekseynp.com/2014/03/28/i-tried-to-sell-open-government-data/"/>
    <updated>2014-03-28T19:13:28-07:00</updated>
    <id>http://alekseynp.com/2014/03/28/i-tried-to-sell-open-government-data</id>
    <content type="html"><![CDATA[<p>I tried to sell open government data, but I won&rsquo;t be the only one.</p>

<p>In 2013 I wrote a series of data journalism articles on the topic of <a href="http://alekseynp.com/topic-ontario-public-salary-disclosure/">Ontario Public Sector Salary Disclosure</a>. The posts were based on data made available as part of a disclosure initiative in the Canadian province of Ontario. Data was published freely and openly on the Ontario Ministry of Finance website in html and pdf format. I performed a convoluted scraping and processing exercise to build a definitive, unified data set and then performed some analysis in order to identify some interesting headlines.</p>

<p>In late 2013 I was approached by someone who was looking for the data behind my analysis. I agreed to share the data on a few conditions, one of which was non-commercial use that they were unable to meet. And so I offered to sell them the data. Free, open, public data.</p>

<blockquote>Sorry Aleksey. Not going to pay for public data even if private hours went into scraping it.</blockquote>


<p>I knew I had entered a moral landscape with shades of grey on a topic that people would be emotional about.</p>

<p>Government data should be free and open! After all it&rsquo;s our data for us! Well except perhaps in instances where user fees would be a more appropriate measure to spread the cost of data production more proportionally to those who will benefit. And the shades are grey.</p>

<p>My simplest and most direct defense is: My customer knew that the data was freely available, but I still had something to sell them.</p>

<p>Nobody owns the truth, but once they have measured it, processed it, and made it available in a useful format, then they have something that they could sell to you. If I sold you a map based on free government data or delivered you a service based on it, you would accept that.</p>

<p>I wrote about the <a href="http://alekseynp.com/2013/05/27/data-sins-of-the-ontario-ministry-of-finance-public-sector-salary-disclosure/">Data Sins of the Ontario Ministry of Finance – Public Sector Salary Disclosure</a> and <a href="http://alekseynp.com/2013/04/22/current-publishing-of-ontario-sunshine-list-not-good-enough/">Current publishing of Ontario “Sunshine List” not good enough</a>. This data was &ldquo;open&rdquo; but not truly &ldquo;open&rdquo; and it required considerable effort to process before I could analyse it.</p>

<p><strong>ProPublica</strong></p>

<p>Why I wrote this today rather than continue to procrastinate is that I saw this: <a href="http://www.propublica.org/article/introducing-the-propublica-data-store">Introducing the ProPublica Data Store</a></p>

<blockquote>For datasets that are the result of significant expenditures of our time and effort, we&#8217;re charging a reasonable one-time fee: In most cases, it&#8217;s $200 for journalists and $2,000 for academic researchers. Those wanting to use data commercially should reach out to us to discuss pricing.</blockquote>


<p>And relevantly, they have data sets for sale based on data acquired through the American Freedom of Information Act.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Visual Analytics, Democratization of Analysis, and Visual Literacy]]></title>
    <link href="http://alekseynp.com/2014/03/21/visual-analytics-democratization-of-analysis-and-visual-literacy/"/>
    <updated>2014-03-21T19:03:41-07:00</updated>
    <id>http://alekseynp.com/2014/03/21/visual-analytics-democratization-of-analysis-and-visual-literacy</id>
    <content type="html"><![CDATA[<p><a href="http://youtu.be/hbTb62BeTUc">http://youtu.be/hbTb62BeTUc</a></p>

<p>Regarding:<a href="https://www.youtube.com/watch?v=hbTb62BeTUc"> SAP Visual Intelligence: Big Data Example </a>on YouTube</p>

<p>First I had better say the positive things. I appreciate that this is a feature demonstration and not a case study. The presenter goes through the feature set of their product in an admirable and straightforward way. It is an impressive feature set and an impressive product. They are not trying to tell a story, so it is unfair to criticize their story.</p>

<p>Then I can say the &ldquo;but&rdquo;. But! I was struck by the badness of the visualizations. ALL of them! Some highlights are below, but you can watch the video for yourself and see not one well purposed chart.</p>

<p>Sales and quantity sold by product line and quarter at 2:12</p>

<p><a href="http://alekseynp.com/wp-content/uploads/2014/03/sap_video_1.png"><img src="http://alekseynp.com/wp-content/uploads/2014/03/sap_video_1.png" alt="sap_video_1" /></a></p>

<p>Sales and quantity sold by product line and quarter at 3:03</p>

<p><a href="http://alekseynp.com/wp-content/uploads/2014/03/sap_video_4.png"><img src="http://alekseynp.com/wp-content/uploads/2014/03/sap_video_4.png" alt="sap_video_4" /></a></p>

<p>3D Bar Pies (is that even a thing?) at 3:33</p>

<p><a href="http://alekseynp.com/wp-content/uploads/2014/03/sap_video_5.png"><img src="http://alekseynp.com/wp-content/uploads/2014/03/sap_video_5.png" alt="sap_video_5" /></a></p>

<p>Variably-sized pies on a map at 3:57</p>

<p><a href="http://alekseynp.com/wp-content/uploads/2014/03/sap_video_6.png"><img src="http://alekseynp.com/wp-content/uploads/2014/03/sap_video_6.png" alt="sap_video_6" /></a></p>

<p>Sales revenue by city by quarter at 4:52</p>

<p><a href="http://alekseynp.com/wp-content/uploads/2014/03/sap_video_7.png"><img src="http://alekseynp.com/wp-content/uploads/2014/03/sap_video_7.png" alt="sap_video_7" /></a></p>

<p>Sales revenue by city at 6:27</p>

<p><a href="http://alekseynp.com/wp-content/uploads/2014/03/sap_video_8.png"><img src="http://alekseynp.com/wp-content/uploads/2014/03/sap_video_8.png" alt="sap_video_8" /></a></p>

<p>The Visual Analytics movement seeks to democratize data analysis by making it easy to use and highly visual. They want to push analysis up towards senior management and outwards away from the analysts. Which is a laudable goal, but when we get there and people are creating stuff like we see in this video, we will have failed.</p>

<p>This illustrates two major calls to action:</p>

<ol>
<li><p>Designers: How do you design a Visual Analytics product that guides its users to make good data viz design decisions?</p></li>
<li><p>Trainers: Go out there and spread the good word. Time to get <a href="http://visualisingdata.com/">Andy Kirk</a> in to train your organization!</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[American Social Mobility, NPR, and Commentary]]></title>
    <link href="http://alekseynp.com/2014/03/19/american-social-mobility-npr-and-commentary/"/>
    <updated>2014-03-19T21:40:50-07:00</updated>
    <id>http://alekseynp.com/2014/03/19/american-social-mobility-npr-and-commentary</id>
    <content type="html"><![CDATA[<p>Regarding: <a href="http://www.npr.org/blogs/money/2014/03/18/289013884/who-had-richer-parents-doctors-or-arists">Who Had Richer Parents, Doctors Or Artists? : Planet Money : NPR</a></p>

<p>First of all, hats off to them for asking and answering important questions about modern society. The tldr; is that they ask &ldquo;Who had richer parents, journalists or people working in finance? Doctors or artists? More generally: What&rsquo;s the link between household income during childhood and job choice during adulthood?&rdquo; and based on a US government survey, they produced an answer.</p>

<p>They generate two elegant charts to illustrate their findings and I invite you to go to the article and see.</p>

<p>But something irked me about all of it and it took me some time to put my finger on it and ultimately it was the lack of extra commentary.</p>

<p>The first chart is a simple message: Doctors earn a lot more than their parents. Designers a lot less. But this isn&rsquo;t all that surprising given that we know what doctors earn quite a lot of money and designers don&rsquo;t. Even in a purely egalitarian society with 100% social mobility we would expect this. All of this tells us is that there is at least some social mobility. Not all rich doctors are born of rich doctor families.</p>

<p>The second chart is delightfully complex for journalism, but I felt even I, as a trained analyst, was struggling to get the right message out of it. Either I&rsquo;m having an off day, I think too highly of myself, or the readers are going to need some help.</p>

<p>Enter my adjustment:</p>

<p><a href="http://alekseynp.com/wp-content/uploads/2014/03/social_mobility_final.png"><img src="http://alekseynp.com/wp-content/uploads/2014/03/social_mobility_final.png" alt="social_mobility_final" /></a></p>

<p>I&hellip;</p>

<ul>
<li><p>&hellip; expanded the axes to include the full income distribution</p></li>
<li><p>&hellip; labeled the X=Y line for what it was</p></li>
<li><p>&hellip; added a Y=50 line to put the data into better context</p></li>
<li><p>&hellip; hand guestimated a line of best fit for the points so we could see where the United States sits</p></li>
<li><p>&hellip; made an important point about the lack of professions in the region left out of the original graphic</p></li>
</ul>


<p>An important and complicated question to answer is still missing: Nurses, for example, come of households at the 50th percentile. But is this all nurses coming from households all at the 50th percentile? Or nurses coming from households with incomes across the entire range, who on average are at the 50th percentile? If we knew the answer to that, then we would really start to learn about the potential for social mobility.</p>

<p>Ultimately they say that, &ldquo;These graphs aren&rsquo;t intended to answer broader questions about inequality and social mobility.&rdquo; But I don&rsquo;t see why they shouldn&rsquo;t be part of the debate.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My US GDP/”Population” Map]]></title>
    <link href="http://alekseynp.com/2014/02/21/my-us-gdppopulation-map/"/>
    <updated>2014-02-21T13:13:51-08:00</updated>
    <id>http://alekseynp.com/2014/02/21/my-us-gdppopulation-map</id>
    <content type="html"><![CDATA[<p>My US GDP/&ldquo;Population&rdquo; Map</p>

<p><a href="http://alekseynp.com/wp-content/uploads/2014/02/us_east_west_gdp.png"><img src="http://alekseynp.com/wp-content/uploads/2014/02/us_east_west_gdp.png" alt="us_east_west_gdp" /></a></p>

<p>Yes, the quality on this is not super high, but the point is to experiment with an idea.</p>

<p>There has been a lot of controversy floating around about a map.</p>

<ul>
<li><p><a href="http://www.visualisingdata.com/index.php/2014/02/defending-the-incredible-gdp-map/">Visualising Data</a></p></li>
<li><p><a href="http://andywoodruff.com/blog/its-just-a-population-map/">Andrew Woodruff</a></p></li>
<li><p><a href="http://www.thefunctionalart.com/2014/02/the-incredible-gdp-map-that-shows-that.html">The Functional Art</a></p></li>
</ul>


<p>&hellip; to name a few.</p>

<p>The map being this: <a href="http://www.reddit.com/r/MapPorn/comments/1y371s/">http://www.reddit.com/r/MapPorn/comments/1y371s/</a></p>

<p>I have a thing for trying out abstract ideas in spacial analysis. Like:</p>

<ul>
<li><p><a href="http://alekseynp.com/2013/10/14/visualising-shot-location-in-basketball/">NBA Basketball</a></p></li>
<li><p><a href="http://alekseynp.com/2012/08/14/new-project-global-games-regional-sports-london-2012/">Olympic Medals</a></p></li>
</ul>


<p>As a western Canadian, I&rsquo;m very aware of the east-west dynamics in the US and Canada, so I thought I would follow a hunch and make this map. I think it demonstrates nicely how lop-sided the US is.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Left and Right Are Important]]></title>
    <link href="http://alekseynp.com/2014/02/17/left-and-right-are-important/"/>
    <updated>2014-02-17T14:05:54-08:00</updated>
    <id>http://alekseynp.com/2014/02/17/left-and-right-are-important</id>
    <content type="html"><![CDATA[<p>The major consultancies are turning out to be a goldmine for content for demonstrating the potential for improved data visualisation design. Today&rsquo;s winner is PWC, though their error is not terribly sinful and with some effort one could probably find something worse.</p>

<p>This is a demonstration of the power and importance of left-to-right. The original violates two assumptions readers make about left-to-right:</p>

<ol>
<li><p>Things further to the right are later in time</p></li>
<li><p>Things further to the right are higher, bigger, better.</p></li>
</ol>


<p>In the original 2014 is shown to the left of 2013 and the bars are ordered high, medium, low.</p>

<p><a href="http://www.pwc.com/gx/en/asset-management/emerging-trends-real-estate/european-investment-market-forecast.jhtml">The Original</a>:</p>

<p><a href="http://alekseynp.com/wp-content/uploads/2014/02/business-prospects.gif"><img src="http://alekseynp.com/wp-content/uploads/2014/02/business-prospects.gif" alt="business-prospects" /></a></p>

<p>Quick answers to basic questions:</p>

<ul>
<li><p>Are things good or bad? Bad. The bars make a download sloping shape.</p></li>
<li><p>Are things better or worse than last year? Hard to tell.</p></li>
</ul>


<p>What if we simply change the left-to-right ordering of everything?</p>

<p>Left-to-right changes:</p>

<p><a href="http://alekseynp.com/wp-content/uploads/2014/02/business-prospects_v2.gif"><img src="http://alekseynp.com/wp-content/uploads/2014/02/business-prospects_v2.gif" alt="business-prospects_v2" /></a></p>

<p>So now we at least get the impression that things are good before we start studying the labeling in detail. We can also scan across, say, the red bars in order to see how things are changing with time without having to realize that the chart&rsquo;s timeline is backwards.</p>

<p>I also experimented with a line design here:</p>

<p><a href="http://alekseynp.com/wp-content/uploads/2014/02/pwc2.png"><img src="http://alekseynp.com/wp-content/uploads/2014/02/pwc2.png" alt="pwc2" /></a></p>

<ul>
<li><p>Maybe the &ldquo;things are good&rdquo; message is a little muted</p></li>
<li><p>We can certainly see the trends over time, though. Generally flat, but with fewer doubters and a bit more optimists.</p></li>
<li><p>The central story of the graphic becomes the crossed lines in Business headcount. Unfortunately this is probably indicating an error in the original graphic. Scroll up and see the probably incorrect data labels</p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KPMG's Grid of Donuts]]></title>
    <link href="http://alekseynp.com/2014/02/16/kpmgs-grid-of-donuts/"/>
    <updated>2014-02-16T13:04:58-08:00</updated>
    <id>http://alekseynp.com/2014/02/16/kpmgs-grid-of-donuts</id>
    <content type="html"><![CDATA[<p>Another day, another poor visual. Apparently I have a quest to complete against bad visuals from consultancies. Today it&rsquo;s KPMG from this tweet:
<a href="https://twitter.com/KPMG/status/434401678931263488">https://twitter.com/KPMG/status/434401678931263488</a></p>

<p>A 4x4 grid of single-value donuts? Are you kidding?</p>

<p>Like <a href="http://alekseynp.com/2014/02/16/dubious-visuals-in-thought-leadership-from-consultancies/">last time</a>, I will show you how this is bad with simple re-design.</p>

<p>First let&rsquo;s try simply cleaning up the graphic a bit:</p>

<p><a href="http://alekseynp.com/wp-content/uploads/2014/02/BgdOBnbIAAAJgsj-2.png"><img src="http://alekseynp.com/wp-content/uploads/2014/02/BgdOBnbIAAAJgsj-2.png" alt="BgdOBnbIAAAJgsj-2" /></a></p>

<p>It looks better already! If I can improve your graphic by erasing the visual elements that you used to encode your data, this is a very bad sign.</p>

<p>And finally the obligatory bar-chart redesign:</p>

<p><a href="http://alekseynp.com/wp-content/uploads/2014/02/kpmg-data-and-analytics.png"><img src="http://alekseynp.com/wp-content/uploads/2014/02/kpmg-data-and-analytics.png" alt="kpmg data and analytics" /></a></p>

<p>Extra patterns become visible:</p>

<ul>
<li><p>EMEA generally more pessimistic than Americas, Asia-Pacific of Overall</p></li>
<li><p>EMEA substantially less impressed by More sophisticated/granular insights</p></li>
</ul>


<p>KPMG can <a href="http://alekseynp.com/2014/02/16/dubious-visuals-in-thought-leadership-from-consultancies/">join Accenture</a> on the list of consultancies that should practice better data visualisation, especially when generating thought leadership on the topic of data and anatlytics.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Analysis of the Determinants of Selling Price for Vancouver NHL Ticket Auctions]]></title>
    <link href="http://alekseynp.com/2014/02/15/analysis-of-the-determinants-of-selling-price-for-vancouver-nhl-ticket-auctions/"/>
    <updated>2014-02-15T17:11:10-08:00</updated>
    <id>http://alekseynp.com/2014/02/15/analysis-of-the-determinants-of-selling-price-for-vancouver-nhl-ticket-auctions</id>
    <content type="html"><![CDATA[<h1>Introduction</h1>

<p>I&rsquo;ve been scraping a lot lately. If data is the new oil, then it&rsquo;s time I started pumping it. I came across this old project of mine from a statistics course in my masters program, dated December 15, 2006. Stubhub today should be a goldmine of data and I hope appropriate data and/or analysis is being passed to the leagues to help them set prices.</p>

<blockquote>**Analysis of the determinants of selling price for Vancouver NHL ticket auctions**

Data was collected for a one month period of auctions completed on eBay, a popular internet auction site, for tickets to see the Vancouver Canucks play at home.  The data was analyzed to determine a good regression model for the selling price per ticket. The most significant indicator was face value, but auctions by sellers with higher feedback scores and transactions that are completed longer before a game also finished with a higher price per ticket. Noteworthy insignificant factors were number of tickets in a lot, the feedback percentage rating for the seller, and the length of the auction.</blockquote>


<h1>Findings</h1>

<p>My most interesting finding would provide guidance to price setting.</p>

<blockquote>Of most interest is the regular game Upper Bowl IV and Upper Bowl V price categories. The eBay market price difference from Ticketmaster is significant and in the case of Upper Bowl V seats, sizable. This indicates that these tickets are priced well below market demand. It would seem that from the consumer’s perspective Upper Bowl IV and V are indistinguishable and the Canucks should seriously consider charging more for these seats.</blockquote>


<p>I also found that:</p>

<ul>
<li><p>Tickets sold further ahead of game day sold for more. There is a fundamental tension in the market for tickets, a time-valued good (after game day a ticket is worth zero). Some shoppers are will pass on an auction hoping to find a better deal later. Some shoppers are willing to pay more for the certainty of having a ticket now. Some sellers want to unload their tickets now before they become worthless. Some sellers will patiently wait out the buyers in order to fetch a higher price. In my data there was a correlation of 0.3 between minimum bid and time until game day. This is evidence  of patient sellers setting and getting higher prices further ahead of game day.</p></li>
<li><p>Feedback rating was a more important factor than feedback percentage. In the marketplace that I studied where most operators were small-scale with 100% positive feedback, their tickets sold at a higher price if they gathered more positive feedback. On the flip-side, big-time operators with only a little less than 100% feedback did not suffer greatly.</p></li>
<li><p>The number of tickets in a lot did not affect selling price per ticket. You would expect four tickets together to be worth more than two separate pairs, and surely they are, but there was not enough evidence in the data, so it is not a strong impact.</p></li>
<li><p>Length of the auction did not have an effect. Longer auctions presumably get seen by more people and therefore sell for more, unless the market is active enough such that a critical mass of buyers will see auctions of short length.</p></li>
</ul>


<h2></h2>

<h1>How I Did It</h1>

<ol>
<li><p>A scraper was written in C# to collect html pages from eBay of completed auctions for Vancouver Canucks tickets</p></li>
<li><p>Extensive regular expression matching was done on the auction&rsquo;s free text in order to determine what game the tickets were for, how many tickets there were, and other factors like where the seats were in the stadium</p></li>
<li><p>Regression models were tried in order to find a good fit</p></li>
<li><p>Regression coefficients were interpreted to draw conclusions</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Joys of R]]></title>
    <link href="http://alekseynp.com/2014/02/10/joys-of-r/"/>
    <updated>2014-02-10T15:41:19-08:00</updated>
    <id>http://alekseynp.com/2014/02/10/joys-of-r</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been coding a lot in R lately. Just thought I&rsquo;d share one example of the insanity.</p>

<p>myList = list(&ldquo;a&rdquo;,&ldquo;b&rdquo;)<br/>
myList[1] == &ldquo;a&rdquo;<br/>
class(myList[1]) == class(&ldquo;a&rdquo;)<br/>
myList[[1]] == &ldquo;a&rdquo;<br/>
class(myList[[1]]) == class(&ldquo;a&rdquo;)<br/>
class(myList)<br/>
class(myList[1])<br/>
class(myList[[1]])<br/>
class(&ldquo;a&rdquo;)</p>

<p>Returns&hellip;</p>

<blockquote><p>myList = list(&ldquo;a&rdquo;,&ldquo;b&rdquo;)<br/>
myList[1] == &ldquo;a&rdquo;<br/>
[1] TRUE<br/>
class(myList[1]) == class(&ldquo;a&rdquo;)<br/>
[1] FALSE<br/>
myList[[1]] == &ldquo;a&rdquo;<br/>
[1] TRUE<br/>
class(myList[[1]]) == class(&ldquo;a&rdquo;)<br/>
[1] TRUE<br/>
class(myList)<br/>
[1] &ldquo;list&rdquo;<br/>
class(myList[1])<br/>
[1] &ldquo;list&rdquo;<br/>
class(myList[[1]])<br/>
[1] &ldquo;character&rdquo;<br/>
class(&ldquo;a&rdquo;)<br/>
[1] &ldquo;character&rdquo;</p></blockquote>

<p>Generally weird but reasonable, except  that myList[1] == &ldquo;a&rdquo; is TRUE even though they are not of the same class.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Outliers Are Easy to See]]></title>
    <link href="http://alekseynp.com/2013/12/15/outliers-are-easy-to-see/"/>
    <updated>2013-12-15T10:37:31-08:00</updated>
    <id>http://alekseynp.com/2013/12/15/outliers-are-easy-to-see</id>
    <content type="html"><![CDATA[<p>Quick and fun. First step of data analysis. Look at your data.</p>

<p>Outliers.</p>

<p><a href="http://alekseynp.com/wp-content/uploads/2013/12/outliers.png"><img src="http://alekseynp.com/wp-content/uploads/2013/12/outliers.png" alt="outliers" /></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Average Shot Locations in the Paint and at Midrange]]></title>
    <link href="http://alekseynp.com/2013/11/07/average-shot-locations-in-the-paint-and-at-midrange/"/>
    <updated>2013-11-07T16:07:15-08:00</updated>
    <id>http://alekseynp.com/2013/11/07/average-shot-locations-in-the-paint-and-at-midrange</id>
    <content type="html"><![CDATA[<p>Yet another post on visualising and analysing NBA shot location data using location averaging methods.</p>

<p>Previously I have shown averages by team for all shots taken. What about shots taken by zone? Consider the following two charts:</p>

<p><a href="http://alekseynp.com/wp-content/uploads/2013/11/avg_inthepaint_teams.png"><img src="http://alekseynp.com/wp-content/uploads/2013/11/avg_inthepaint_teams.png" alt="avg_inthepaint_teams" /></a></p>

<p><a href="http://alekseynp.com/wp-content/uploads/2013/11/avg_midrange_teams.png"><img src="http://alekseynp.com/wp-content/uploads/2013/11/avg_midrange_teams.png" alt="avg_midrange_teams" /></a></p>

<p>Now we can see a level of detail that we couldn&rsquo;t in the average of all shots.</p>

<p>Previously:</p>

<p><a href="http://alekseynp.com/2013/11/05/more-on-averages-shot-locations/"><img src="http://alekseynp.com/wp-content/uploads/2013/11/for_and_against_640w1-150x150.png" alt="for_and_against_640w" /></a></p>

<p>Observations:</p>

<ul>
<li><p>Previously we saw that the GSW were the longest shooters in the league and indeed they were also long shooters in the paint and at midrange. It is not simply that the GSW take a lot of 3s.</p></li>
<li><p>Other teams like the NYK take shots from close up in both the paint and at midrange. They were generally longer shooters in the previous analysis, suggesting that they balance those close 2-pts with many 3-pt attempts</p></li>
<li><p>Previously we saw DEN as the closest shooters overall, and indeed they appear to be close shooters in the paint and moderately close shooters at midrange.</p></li>
</ul>

]]></content>
  </entry>
  
</feed>
