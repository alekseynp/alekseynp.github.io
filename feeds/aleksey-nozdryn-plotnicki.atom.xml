<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Aleksey Nozdryn-Plotnicki - Aleksey Nozdryn-Plotnicki</title><link href="/" rel="alternate"></link><link href="/feeds/aleksey-nozdryn-plotnicki.atom.xml" rel="self"></link><id>/</id><updated>2019-02-11T12:00:00+00:00</updated><entry><title>Paper Summary: mixup: Beyond Empirical Risk Minimization</title><link href="/mixup-paper-summary.html" rel="alternate"></link><published>2019-02-11T12:00:00+00:00</published><updated>2019-02-11T12:00:00+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2019-02-11:/mixup-paper-summary.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is summary of &lt;a href="https://arxiv.org/abs/1710.09412"&gt;mixup: Beyond Empirical Risk Minimization&lt;/a&gt; for the &lt;a href="https://www.meetup.com/LearnDataScience/"&gt;Vancouver Data Science Reading Group&lt;/a&gt;. It was a poster paper at ICLR 2018. &lt;/p&gt;
&lt;p&gt;The paper authors are: Hongyi Zhang (MIT), Moustapha Cisse (FAIR), Yann N. Dauphin (FAIR), David Lopez-Paz (FAIR)&lt;/p&gt;
&lt;p&gt;There's code!: &lt;a href="https://github.com/facebookresearch/mixup-cifar10"&gt;CIFAR10&lt;/a&gt; and &lt;a href="ttps://github.com/hongyi-zhang/mixup"&gt;more code from the …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is summary of &lt;a href="https://arxiv.org/abs/1710.09412"&gt;mixup: Beyond Empirical Risk Minimization&lt;/a&gt; for the &lt;a href="https://www.meetup.com/LearnDataScience/"&gt;Vancouver Data Science Reading Group&lt;/a&gt;. It was a poster paper at ICLR 2018. &lt;/p&gt;
&lt;p&gt;The paper authors are: Hongyi Zhang (MIT), Moustapha Cisse (FAIR), Yann N. Dauphin (FAIR), David Lopez-Paz (FAIR)&lt;/p&gt;
&lt;p&gt;There's code!: &lt;a href="https://github.com/facebookresearch/mixup-cifar10"&gt;CIFAR10&lt;/a&gt; and &lt;a href="ttps://github.com/hongyi-zhang/mixup"&gt;more code from the authors&lt;/a&gt; and &lt;a href="https://github.com/tensorpack/tensorpack/tree/master/examples/ResNet#cifar10-preact18-mixuppy"&gt;TF code linked by the authors&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;Why should you care?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;mixup allows a new state-of-the-art performance in the CIFAR-10, CIFAR100, and ImageNet-2012 image classification datasets &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The only way practitioners think about doing classification in deep learning is by minimizing categorical cross entropy loss against one hot encoded vectors. This paper presents a viable alternative.&lt;/p&gt;
&lt;p&gt;What is Empirical Risk Minimization?
The authors actually do a very good job of building up expected risk, empirical risk, and finally vicinal risk. They are actually quite generous to the reader and walk us through what is probably boring elementary theory for some research audiences.&lt;/p&gt;
&lt;p&gt;The key takeaway here is that data augmentation isn’t just something that engineers do in order to make their data set feel bigger or something that you bolt on to your training loop. Data augmentation is actually a fundamental component of how we formulate our problem. There are other ways of formulating the problem whose implementations might look like data augmentation, and mixup is one of them.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;What is it?&lt;/h2&gt;
&lt;h3&gt;A Cheap Engineering Hack&lt;/h3&gt;
&lt;p&gt;Instead of training your model to output a class given a training example, randomly interpolate between two training examples and train your model to output the same interpolated class. In a CIFAR10 example, we might randomly draw a interpolation of 0.2. Take a photo of an airplane and a photo of an automobile and mix then 20%/80% pixel-wise on their RGB values. Compute your loss against a target out vector with a 20% probability for airplane and 80% for automobile. Done. Easy to do, if a bit bizarre to do with images.&lt;/p&gt;
&lt;p&gt;Mixup gets better results and it is a drop-in replacement in normal training routine with nearly zero cost or complexity.&lt;/p&gt;
&lt;h3&gt;A Wise Reformulation&lt;/h3&gt;
&lt;p&gt;In a classification problem we can think of our training data as being points in a high dimensional space. With mixup we re-formulate the problem so that our data is a fully connected graph where the vectors between training examples define the way in which our function outputs change.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;What is the impact?&lt;/h2&gt;
&lt;p&gt;We end up with a lot of training examples that would never arise in the real world, but we also gain access to a rich vicinity. Most of images that are 20% of the way from an automobile to an airplane are bizarre unnatural images or images with a little structured noise. However, almost all of those images should be classified as automobile (argmax([0.8, 0.2]) and some of those images may actually start to look a little like an airplane (maybe even 20%).&lt;/p&gt;
&lt;p&gt;Interpolating spectrograms for audio classification like in their section 3.3 makes a whole lot more sense than images in sections 3.1 and 3.2.&lt;/p&gt;
&lt;p&gt;They:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Eek out small improvements in classification error in the ImageNet 2012 benchmark&lt;/li&gt;
&lt;li&gt;Improved classification error on CIFAR-10 and CIFAR-100 image classification benchmarks&lt;/li&gt;
&lt;li&gt;Show other interesting results, though not as comprehensive or compelling&lt;ul&gt;
&lt;li&gt;Demonstrate the potential to improve spectrogram classification&lt;/li&gt;
&lt;li&gt;Demonstrate frankly amazing resiliency to corrupted labels&lt;/li&gt;
&lt;li&gt;Demonstrate curious increased resiliency to adversarial attacks with one very interesting exception.&lt;/li&gt;
&lt;li&gt;Demonstrate improvements on tabular data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Their GAN idea is nice, but their results are really weak and I don’t think worth discussing&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;About Resiliency to Adversarial Attacks&lt;/h3&gt;
&lt;p&gt;I think the behaviour they demonstrate in Figure 2 makes their network &lt;em&gt;more&lt;/em&gt; vulnerable to the white box iterative fast gradient sign method (I-FGSM) attack. This is not a criticism of their method, as white box attacks are not too concerning in practice. They do however mislead with their black box results, because they attack the ERM model and attempt to transfer to the mixup model. They should have also attacked a separately trained mixup model and attempted transfer to both.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;Big Obvious Question&lt;/h2&gt;
&lt;p&gt;Why not interpolate in feature space rather than input space? Surely it makes more sense to mix features with some potential semantic meaning rather than the pixels of an image? They address this essentially by trying it out in their ablation studies and surprisingly their choice leads to the best result.&lt;/p&gt;</content></entry><entry><title>Paper Summary: PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title><link href="/pointnet-paper-summary.html" rel="alternate"></link><published>2018-09-27T12:00:00+00:00</published><updated>2018-09-27T12:00:00+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2018-09-27:/pointnet-paper-summary.html</id><summary type="html">&lt;style&gt;
    .MathJax {
        font-size: 1em;
    }
&lt;/style&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is summary of &lt;a href="https://arxiv.org/abs/1612.00593"&gt;PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation&lt;/a&gt; for the &lt;a href="https://www.meetup.com/LearnDataScience/"&gt;Vancouver Data Science Reading Group&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The paper authors are: Charles R. Qi, Hao Su, Kaichun Mo, Leonaidas J. Guibas&lt;/p&gt;
&lt;p&gt;There's code!: &lt;a href="https://github.com/charlesq34/pointnet"&gt;https://github.com/charlesq34/pointnet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There's a lot …&lt;/p&gt;</summary><content type="html">&lt;style&gt;
    .MathJax {
        font-size: 1em;
    }
&lt;/style&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is summary of &lt;a href="https://arxiv.org/abs/1612.00593"&gt;PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation&lt;/a&gt; for the &lt;a href="https://www.meetup.com/LearnDataScience/"&gt;Vancouver Data Science Reading Group&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The paper authors are: Charles R. Qi, Hao Su, Kaichun Mo, Leonaidas J. Guibas&lt;/p&gt;
&lt;p&gt;There's code!: &lt;a href="https://github.com/charlesq34/pointnet"&gt;https://github.com/charlesq34/pointnet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There's a lot going on in this paper. I highlight the most important parts below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Deep Learning on Unordered Sets&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Point Clouds&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Architecture&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Implementation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why does it work on point clouds?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Other Comments&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;Deep Learning on Unordered Sets&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;This is a powerful idea that is useful for anyone working in Machine Learning, not at all limited to point clouds or computer vision applications.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In ML we should be very familiar with learning a function &lt;span class="math"&gt;\(f(x)\rightarrow y\)&lt;/span&gt; that takes some vector &lt;span class="math"&gt;\(x\)&lt;/span&gt; and creates a desired output &lt;span class="math"&gt;\(y\)&lt;/span&gt;, be it a regression task or a classification task. But what about a function on a set &lt;span class="math"&gt;\(f(\left \{x_1,x_2,...,x_n\right \})\rightarrow y\)&lt;/span&gt;? The critical detail is that this function should be invariant to permutations of its inputs, &lt;span class="math"&gt;\(f(\left \{x_1,x_2,...,x_n\right \})= f(\left \{x_n,x_2,...,x_1\right \})\)&lt;/span&gt;, because indeed the set's meaning has not changed.&lt;/p&gt;
&lt;p&gt;A typical ML paradigm expects a single vector input. Your options are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Sort input into a canonical order. Invent some ordering and concatenate the vectors to a single &lt;span class="math"&gt;\(x\)&lt;/span&gt;. This is clearly suboptimal as our model will not understand the shared nature of elements of &lt;span class="math"&gt;\(x\)&lt;/span&gt; and will treat each as a unique feature.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Augment the training data with all kinds of permutations. This is an ugly hack, and utterly impossible if the set is large.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The author's approach.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The idea is to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create an &lt;span class="math"&gt;\(h(x)\)&lt;/span&gt; and apply it to every element of the set.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apply a symmetric function &lt;span class="math"&gt;\(g(h(x_1),h(x_2),...,h(x_n))\rightarrow y\)&lt;/span&gt;. The magically simple fact is that &lt;em&gt;mean&lt;/em&gt; and &lt;em&gt;max&lt;/em&gt; are symmetric functions, and therefore &lt;code&gt;max_pool&lt;/code&gt; and &lt;code&gt;average_pool&lt;/code&gt; are symmetric functions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The "Deep Learning" comes in when the authors implement the &lt;span class="math"&gt;\(h()\)&lt;/span&gt; as a multi-layer perceptron (deep neural network) and implement &lt;span class="math"&gt;\(g()\)&lt;/span&gt; as a &lt;code&gt;max&lt;/code&gt; followed by another multi-layer perceptron.&lt;/p&gt;
&lt;p&gt;Why this works on point clouds is hard to wrap your head around. Suppose instead we wanted to take a team of developers and predict if they are likely to ship a release on time. We apply &lt;span class="math"&gt;\(h()\)&lt;/span&gt; to each developer to compute a feature vector &lt;span class="math"&gt;\(h(x_i)\)&lt;/span&gt; that describes the developer. If we apply a feature-wise &lt;code&gt;max&lt;/code&gt; across the feature vectors, we get a single feature vector signature for the development team that did not depend on the set order. Finally we process that team feature to make our final prediction. &lt;span class="math"&gt;\(g(max(\left \{h(x_1),h(x_2),...,h(x_n)\right \}))\rightarrow y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the Zillow Kaggle competition I experimented with PointNets. When predicting the value of a house, we could use statistical aggregate metrics about the neighbourhood it falls in, but we could also consider the k nearest houses as an unordered set that we could use to make predictions from. In a sense, the statistical measures like mean, median, standard deviation, etc. are engineered features and a DNN learning on the set has a chance of learning its own features. That may sound esoteric, but it is the classic story when using DNNs for computer vision applications vs. how those tasks were solved in the past.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;Point Clouds&lt;/h2&gt;
&lt;p&gt;Classifying 2D images has been done to death, but what about 3D data? Most of the 3D graphics we see in everyday life are triangular meshes. Millions of triangles rendered on a screen to make your video game or movie. However, the typical data format that comes from a 3D sensor is a point cloud. Some kind of sensor measures the 3D environment and generates an unordered list of &lt;span class="math"&gt;\((x, y, z)\)&lt;/span&gt; coordinates. If you look at the raw data it doesn't look like much, but if you did a &lt;code&gt;plot3d&lt;/code&gt; on it, you would be able to see it and guess what it is.&lt;/p&gt;
&lt;p&gt;In particular the authors run experiments on &lt;em&gt;ModelNet40&lt;/em&gt; a benchmark dataset not unlike CIFAR-10 or CIFAR-100. The native data format for ModelNet40 is actually a triangular mesh, so converting it to point clouds is a little artificial, but it makes for a worthwhile benchmark to do research against. The authors achieve very good results on the benchmark with their approach.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;Architecture&lt;/h2&gt;
&lt;p&gt;Their architecture is at the top of page 3. For a first time reader of this paper, the "input transform" and "feature transform" from the "joint alignment networks" are best ignored and not important. They contribute very little to performance in the end. One should also understand the classification case before studying the segmentation.&lt;/p&gt;
&lt;p&gt;The basic classification architecture is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Input points of shape &lt;code&gt;(n, 3)&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MLP fully connected deep neural network with sizes 64, 64, 64, 128, 1024 applied to each point individually mapping &lt;span class="math"&gt;\(R^3\rightarrow R^{1024}\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Max pool from shape &lt;code&gt;(n, 1024)&lt;/code&gt; to a feature vector of shape &lt;code&gt;(1024,)&lt;/code&gt; &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MLP fully connected with size 512, 256, k where k is the number of classes mapping &lt;span class="math"&gt;\(R^{1024}\rightarrow R^k\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;p&gt;It's worth seeing their clever implementation &lt;a href="https://github.com/charlesq34/pointnet/blob/master/models/pointnet_cls.py"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An input batch is prepared as an "image" of shape &lt;code&gt;(batch_size, 1, n, 3)&lt;/code&gt; (channels first notation)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A 2D convolution with 64 kernels of size &lt;code&gt;(1, 3)&lt;/code&gt; with no padding is applied. This is equivalent to applying the same 64 neuron hidden layer to every row of the "image" or in fact the same 64 neuron hidden layer to every point in the cloud. The output is of shape &lt;code&gt;(batch_size, 64, n, 1)&lt;/code&gt; or an "image" with 64 channels, 1 pixel wide and n pixels tall.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After that 2D convolutions with filters 64, 64, 128, and 1024 of size &lt;code&gt;(1,1)&lt;/code&gt; process every point individually. The net effect is that the same MLP(64, 64, 64, 128, 1024), our &lt;span class="math"&gt;\(h()\)&lt;/span&gt; is applied to every point.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally a global max pool reduces &lt;code&gt;(batch_size, 1024, n, 1)&lt;/code&gt; to &lt;code&gt;(batch_size, 1024)&lt;/code&gt; and fully connected layers are used for classification.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You could also implement this as 1D convolutions starting with a input of shape &lt;code&gt;(batch_size, 3, n)&lt;/code&gt; (channels first notation) and indeed I do in my own work.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;Why does it work on point clouds?&lt;/h2&gt;
&lt;p&gt;Each of the 1024 output features from the &lt;span class="math"&gt;\(h()\)&lt;/span&gt; can be thought of as point detectors. These numbers are derived only from 3 inputs! Some of these feature are excited by points that are very far from the origin. Some are excited by points below the origin. Some are excited by points in an arc in front of the origin. Before the max pool, each input point has excited some of the feature space elements, and after the max pool we have a summary of how the point cloud excited the features. A summary of where the points are.&lt;/p&gt;
&lt;p&gt;Maybe the visualizations in figure 19 on page 15 would help.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;Other Comments&lt;/h2&gt;
&lt;h3&gt;Extensions&lt;/h3&gt;
&lt;p&gt;The major limitation of their architecture is that it makes no use of local structure and cannot learn hierarchical features. There is a &lt;a href="https://arxiv.org/abs/1706.02413"&gt;
PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space&lt;/a&gt; that is worth looking up.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Comparison to VoxNet&lt;/h3&gt;
&lt;p&gt;Why, in Supplementary B, do they make a point of demonstrating their superior robustness vs. VoxNet?&lt;/p&gt;
&lt;p&gt;Voxels are an alternative approach for classifying 3D objects. In the case of ModelNet40, the triangular meshes are converted to a 3D grid where each cell in the grid is occupied or not. Think about converting the 3D models to LEGO. After that, 3D CNNs can be applied in an analogous way to how 2D CNNs are applied for image classification. Converting the ModelNet40 triangular meshes to point clouds for this paper was an unfortunate necessity, but it's worth noting that voxel based approaches are also inelegant. Ultimately we should strive to process data in its native format. In the case of voxels, they are complete garbage if your input data is a sparse point cloud, or a point cloud that is in places sparse. This is because a sparse pount cloud converted to a voxel grid will provide only a few occupied cells. A 3DNN will then be mostly convolving across zeros and will struggle to learn anything. It's worth noting that this sparseness is actually a &lt;em&gt;very&lt;/em&gt; common phenomenon in real world point clouds. They tend to be stupidly dense in some areas and frustratingly sparse in others as a consequence of the technology that gathers them.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Effect of Bottleneck Dimension and Number of Input Points&lt;/h3&gt;
&lt;p&gt;The Supplementary F section is interesting to a practitioner and figure 15 in particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;They achieve incredible performance with only 64 input points. A human looking at a &lt;code&gt;Plot3D&lt;/code&gt; with only 64 points would struggle to classify the objects. In that sense, this network is superhuman.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is important that the feature size be relatively large. The "bottleneck size" that is generally presented as 1024 in the architecture. The PointNet is roughly equivalent to a 7 layer CNN operating on a 32x32x3 image. 1024 would be very large in that case. This highlights the brute force nature of this architecture and the lack of local structure/hierarchical feature exploitation. The message for a practitioner is to make this layer larger than you might otherwise.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>We won the 2018 ISIC Skin Lesion Classification Challenge!</title><link href="/2018-isic-classification-challenge-winner.html" rel="alternate"></link><published>2018-09-01T12:00:00+00:00</published><updated>2018-09-01T12:00:00+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2018-09-01:/2018-isic-classification-challenge-winner.html</id><summary type="html">&lt;p&gt;As part of the &lt;a href="https://www.metaoptima.com/"&gt;MetaOptima&lt;/a&gt; R&amp;amp;D team,  won the ISIC 2018 Disease Classification Challenge put on by the &lt;a href="https://isic-archive.com/"&gt;International Skin Imaging Collaboration&lt;/a&gt;. The task was to design and build a system to autmoatically diagnose a photo of a mole as one of seven diseases: Melanoma, melanocytic nevus, basal cell …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As part of the &lt;a href="https://www.metaoptima.com/"&gt;MetaOptima&lt;/a&gt; R&amp;amp;D team,  won the ISIC 2018 Disease Classification Challenge put on by the &lt;a href="https://isic-archive.com/"&gt;International Skin Imaging Collaboration&lt;/a&gt;. The task was to design and build a system to autmoatically diagnose a photo of a mole as one of seven diseases: Melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis, benign keratosis, dermatofibroma, and vascular lesion.&lt;/p&gt;
&lt;p&gt;We won with a score of 88.5%, a significant 2.9% better than the second best team as visible on the &lt;a href="href=" title="https://challenge2018.isic-archive.com/leaderboards/"&gt;Task 3 Leaderboard&lt;/a&gt;&lt;/p&gt;</content></entry><entry><title>Generating Targeted Universal Adversarial Images from Discriminative Networks: Diagnostic Potential</title><link href="/adversarial-images-diagnostics.html" rel="alternate"></link><published>2018-01-20T20:00:00+00:00</published><updated>2018-01-20T20:00:00+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2018-01-20:/adversarial-images-diagnostics.html</id><summary type="html">&lt;style&gt;
    .MathJax {
        font-size: 4em;
    }
&lt;/style&gt;

&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Analysing, visualizing, diagnosing, and interpreting neural networks is famously difficult. In my recent work in adversarial images, images that have been modified by an adversary in order to fool an image classifier (discriminator), I have discovered a new tool for this task. Similar things have been …&lt;/p&gt;</summary><content type="html">&lt;style&gt;
    .MathJax {
        font-size: 4em;
    }
&lt;/style&gt;

&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Analysing, visualizing, diagnosing, and interpreting neural networks is famously difficult. In my recent work in adversarial images, images that have been modified by an adversary in order to fool an image classifier (discriminator), I have discovered a new tool for this task. Similar things have been tried before, but I don't believe with this success.&lt;/p&gt;
&lt;p&gt;For a given DNN (deep neural network) classifier and a target class, it is possible to create a single change that when added to any image, it fools that network with high probability. It is possible to create this &lt;strong&gt;only&lt;/strong&gt; with the trained neural network and the data upon which it was trained. In practice it is not necessary to have the training data. Any approximately similar data should do.&lt;/p&gt;
&lt;p&gt;Here are a few such changes for Google's Inception v3 trained on imagenet:&lt;/p&gt;
&lt;div class="row"&gt;
&lt;div id=33 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/33.png" alt="loggerhead, loggerhead turtle, Caretta caretta" /&gt;&lt;br /&gt;
&lt;a href="#33"&gt;loggerhead, loggerhead turtle, Caretta caretta&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;div id=34 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/34.png" alt="leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea" /&gt;&lt;br /&gt;
&lt;a href="#34"&gt;leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;div id=35 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/35.png" alt="mud turtle" /&gt;&lt;br /&gt;
&lt;a href="#35"&gt;mud turtle&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;div class="row"&gt;
&lt;div id=36 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/36.png" alt="terrapin" /&gt;&lt;br /&gt;
&lt;a href="#36"&gt;terrapin&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;div id=37 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/37.png" alt="box turtle, box tortoise" /&gt;&lt;br /&gt;
&lt;a href="#37"&gt;box turtle, box tortoise&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;These images are vibrant, clear, interpretable, and obviously highly related to the target class. We can study these and see what the neural network has learned about each turtle class.&lt;/p&gt;
&lt;p&gt;This is not just constrained to turtles, of course, but can be done with any imagenet class. I have created &lt;a href="pages/inception-v3-targeted-universal-attacks-page-6.html"&gt;1,000 such images, one for every class&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Large cats:
&lt;div class="row"&gt;
&lt;div id=288 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/288.png" alt=leopard, Panthera pardus /&gt;&lt;br /&gt;
&lt;a href="#288"&gt;leopard, Panthera pardus&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;div id=289 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/289.png" alt=snow leopard, ounce, Panthera uncia /&gt;&lt;br /&gt;
&lt;a href="#289"&gt;snow leopard, ounce, Panthera uncia&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;div id=290 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/290.png" alt=jaguar, panther, Panthera onca, Felis onca /&gt;&lt;br /&gt;
&lt;/div&gt;&lt;br /&gt;
&lt;div class="row"&gt;
&lt;a href="#290"&gt;jaguar, panther, Panthera onca, Felis onca&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;div id=291 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/291.png" alt=lion, king of beasts, Panthera leo /&gt;&lt;br /&gt;
&lt;a href="#291"&gt;lion, king of beasts, Panthera leo&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;div id=292 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/292.png" alt=tiger, Panthera tigris /&gt;&lt;br /&gt;
&lt;a href="#292"&gt;tiger, Panthera tigris&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;div id=293 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/293.png" alt=cheetah, chetah, Acinonyx jubatus /&gt;&lt;br /&gt;
&lt;a href="#293"&gt;cheetah, chetah, Acinonyx jubatus&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h1&gt;Diagnosis&lt;/h1&gt;
&lt;p&gt;Studying the images, it is possible to identify problems with the network, or perhaps with the dataset that it was trained on.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;div class="row"&gt;
&lt;div id=664 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/664.png" alt="monitor" /&gt;&lt;br /&gt;
&lt;a href="#664"&gt;monitor&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Is that Chinese on the monitor? No it's not actual Chinese, but it is strange proto-Chinese that has been generated by the process. The network has learned that Chinese written over the image is a very strong indicator that it is a monitor. If you look at the imagenet training data, you will see that a large portion of the monitor images have been sourced from Alibaba and they have Chinese overlayed on the image.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;div class="row"&gt;
&lt;div id=670 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/670.png" alt=motor scooter, scooter /&gt;&lt;br /&gt;
&lt;a href="#670"&gt;motor scooter, scooter&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;div id=774 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/774.png" alt=sandal /&gt;&lt;br /&gt;
&lt;a href="#774"&gt;sandal&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;div id=793 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/793.png" alt=shower cap /&gt;&lt;br /&gt;
&lt;a href="#793"&gt;shower cap&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Monitor is not the only object with this problem, though it is perhaps the most striking.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;div class="row"&gt;
&lt;div id=978 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/978.png" alt=seashore, coast, seacoast, sea-coast /&gt;&lt;br /&gt;
&lt;a href="#978"&gt;seashore, coast, seacoast, sea-coast&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Tourism is big business, and so a lot of the seashore images have English written on them, hence the funny proto-English above.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;div class="row"&gt;
&lt;div id=96 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/96.png" alt="toucan" /&gt;&lt;br /&gt;
&lt;a href="#96"&gt;toucan&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;There is a distinct cage or net visible for the toucan. From a quick glance at the training data, about half of toucan images include some cage or net. This is not something that we would want our network to learn if we were training it to identify toucans in the wild!&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;div class="row"&gt;
&lt;div id=78 class="col-md-4"&gt;&lt;center&gt;
&lt;img src="/images/incv3_univ/78.png" alt="tick" /&gt;&lt;br /&gt;
&lt;a href="#78"&gt;tick&lt;/a&gt;&lt;/center&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Clearly the network has learned a lot about &lt;strong&gt;where&lt;/strong&gt; you find ticks, not just what they look like. Here is a tuck buried in some long hair.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1&gt;Creating Targeted Universal Images: The Math&lt;/h1&gt;
&lt;p&gt;Creating a targeted universal image looks a lot like the routine for training a neural network, but instead of training the network to make the correct output, we train an image to make the network output the target instead of what is correct.&lt;/p&gt;
&lt;p&gt;We use the crossy entropy loss, as was used to train the network.&lt;/p&gt;
&lt;p&gt;The goal is to maximize the expected output for the target, &lt;span class="math"&gt;\(p_{target}\)&lt;/span&gt;, when the change &lt;span class="math"&gt;\(\delta\)&lt;/span&gt; is made to a randomly selected image &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\max_{\delta}E_x[p_{target}]
$$&lt;/div&gt;
&lt;p&gt;We constrain the &lt;span class="math"&gt;\(\delta\)&lt;/span&gt; to be to be no more than some &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; for each pixel value. This is the infinity norm constraint.&lt;/p&gt;
&lt;div class="math"&gt;$$
\lvert\lvert\delta\lvert\lvert_\infty \leq \epsilon
$$&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Change of Variable&lt;/h3&gt;
&lt;p&gt;We perform a change of variable in order to do unconstrained optimization. For each pixel value &lt;span class="math"&gt;\(i\)&lt;/span&gt; we define a &lt;span class="math"&gt;\(\delta_i\)&lt;/span&gt; function that maps a new variable &lt;span class="math"&gt;\(w_i \in [-\infty,\infty]\)&lt;/span&gt; to &lt;span class="math"&gt;\(\delta_i\)&lt;/span&gt; values that obey the constraint:&lt;/p&gt;
&lt;div class="math"&gt;$$
\delta_i(w_i) = \epsilon\cdot tanh(w_i)
$$&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\(tanh(w) \in (-1,1)\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;then &lt;span class="math"&gt;\(\delta_i(w_i) \in (-\epsilon,\epsilon)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can them perform unconstrained stochastic gradient descent with optimizers like Adam.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Optimization&lt;/h3&gt;
&lt;p&gt;Given a random &lt;span class="math"&gt;\(x\)&lt;/span&gt; from the dataset, we compute &lt;span class="math"&gt;\(p_{target} = f_{target}(x + \delta(w))\)&lt;/span&gt; where &lt;span class="math"&gt;\(f\)&lt;/span&gt; is the target neural network (i.e. inception v3).&lt;/p&gt;
&lt;p&gt;Then we compute the gradient &lt;span class="math"&gt;\(\frac{\partial p_{target}}{\partial w}\)&lt;/span&gt;, pass it to the Adam optimizer, and take a step to maximize &lt;span class="math"&gt;\(p_{target}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Improving Results with Augmentations&lt;/h3&gt;
&lt;p&gt;Deep Neural Networks are very easily fooled. Unless we do something, our optimization routine is likely to converge on images that exploit this weakness, rather than utilize the true properties of the target class. You can see some of this in the images that I have created, though I have taken steps to mitigate it. For example, the center of the big cat images above are largely trained to exploit DNN weaknesses rather than look like big cats.&lt;/p&gt;
&lt;p&gt;We insert differentiable image augmentations between the changed image and the target network. &lt;span class="math"&gt;\(\delta\)&lt;/span&gt; is calculated and added to the image. The result is clipped to be a valid image &lt;span class="math"&gt;\(x + \delta \in [0,255]\)&lt;/span&gt;. The image is randomly flipped horizontally, blurred with a Gaussian kernel, and randomly cropped. Finally the image is bilinearly resized back up to &lt;code&gt;(299,299)&lt;/code&gt; as expected by inception v3.&lt;/p&gt;
&lt;p&gt;The augmetations can be thought of as a function &lt;span class="math"&gt;\(A(X)\)&lt;/span&gt; and we can consider our optimization to be seeking to maximize:
&lt;/p&gt;
&lt;div class="math"&gt;$$
\max_{w}E_x[f_{target}(A(x + \delta(w)))]
$$&lt;/div&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;When I began working with adversarial images I felt that therin lay some essential truth behind the functioning of neural networks. What I demonstrate here is a technique that offers diagnostic power in assessing a DNN CNN image classifier.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>Paper: An infinity norm constrained attack for the NIPS 2017 Competition Track</title><link href="/nips2017-adversarial.html" rel="alternate"></link><published>2017-12-07T20:00:00+00:00</published><updated>2017-12-07T20:00:00+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2017-12-07:/nips2017-adversarial.html</id><summary type="html">&lt;p&gt;My teammate, Ross Wightman, and I recently placed 5th in the &lt;a href="https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack"&gt;Targeted Adversarial Attack&lt;/a&gt; and 9th in the &lt;a href="https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack"&gt;Non-targeted Adversarial Attack&lt;/a&gt; competitions hosted on Kaggle as part of the NIPS 2017 competition track.&lt;/p&gt;
&lt;p&gt;We've written a paper and it is &lt;a href="/papers/nips2017-adversarial-paper.pdf"&gt;available here&lt;/a&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;My teammate, Ross Wightman, and I recently placed 5th in the &lt;a href="https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack"&gt;Targeted Adversarial Attack&lt;/a&gt; and 9th in the &lt;a href="https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack"&gt;Non-targeted Adversarial Attack&lt;/a&gt; competitions hosted on Kaggle as part of the NIPS 2017 competition track.&lt;/p&gt;
&lt;p&gt;We've written a paper and it is &lt;a href="/papers/nips2017-adversarial-paper.pdf"&gt;available here&lt;/a&gt;.&lt;/p&gt;</content></entry><entry><title>Where do adversarial images come from?</title><link href="/where-do-adversarial-images-come-from.html" rel="alternate"></link><published>2017-10-29T20:00:00+00:00</published><updated>2017-10-29T20:00:00+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2017-10-29:/where-do-adversarial-images-come-from.html</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;By reading this post, I hope that you will gain an intuitive understanding of adversarial examples. They are not as spooky and magical as they appear at first, and maybe it isn't even surprising that they exist. First, I will quickly cover what I mean by adversarial images. Second …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;By reading this post, I hope that you will gain an intuitive understanding of adversarial examples. They are not as spooky and magical as they appear at first, and maybe it isn't even surprising that they exist. First, I will quickly cover what I mean by adversarial images. Second, I will discuss ImageNet classifiers. Finally I will use a two-dimensional example build an intuitive understanding.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1&gt;What are adversarial images?&lt;/h1&gt;
&lt;p&gt;An adversarial example is a data point that has been manipulated by an adversary with some goal in mind, probably a goal opposite to yours. In the case of an image classifier, it is a natural image, that has been modified by an adversary in order to fool the classifier. The goal is typically either to get the classifier to output a specific class rather than the correct class (a targeted attack) or simply to get the classifier to output the wrong class (a non-targeted attack).&lt;/p&gt;
&lt;p&gt;Fairly recently, everyone was quite surprised to discover that you can easily fool Deep Neural Networks (DNNs) with a very small change to an image. These are changes that a human would not even perceive. We are faced with the possibility that deployed classifiers in the real world could be fooled by an adversary when a human supervisor would not even realize what is happening. For any application with a potential adversary this is a problem, and for any application where there are safety considerations this is a big problem. A surveillance system could be fooled into ignoring exactly the bad people it is looking for. A virus scanner could be similarly challenged. What if pranksters &lt;a href="https://arxiv.org/abs/1602.02697"&gt;fool our self-driving cars into identifying stop signs as yield signs&lt;/a&gt;? The consequences could be tragic.&lt;/p&gt;
&lt;p&gt;Publications on the topic are littered with same-looking images with captions that claim they are classified differently by a classifier.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;center&gt;
&lt;div class="container"&gt;
&lt;img alt="OpenAI Example" src="/images/where_do_adversarial/adversarial_img_1.png"&gt;&lt;/p&gt;
&lt;p&gt;Figure: An &lt;a href="https://blog.openai.com/adversarial-example-research/"&gt;example from the people at OpenAI&lt;/a&gt;
&lt;/div&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;This is an academic field with real world importance, but I haven't yet seen any well articulated attack scenarios. This means that we tend to speak generally about an adversary changing the input to our classifier without specifying how they did it or what their limitations were. If an adversary can change every pixel of the image to whatever they want, it's little surprise that they can fool a classifier. They could simply substitute any image they like. Therefore it only makes sense to talk about adversarial examples in the context of small changes. Typically you either try to fool a classifier while minimizing the change, or alternatively you try to fool it as much as possible within some constraint as to how large the change is allowed to be.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1&gt;What are ImageNet classifiers doing?&lt;/h1&gt;
&lt;p&gt;For simplicity, we will talk about the generation of models that take RGB images with width and height both 299 pixels as inputs.&lt;/p&gt;
&lt;p&gt;Neural networks generally, and ImageNet classifiers in particular, are mapping vector spaces. An input image is just a collection of numbers, a red, green, and blue value for each pixel. An image can be thought of as &lt;span class="math"&gt;\( 299*299*3 = 268203\)&lt;/span&gt; different variables, each taking a value in &lt;span class="math"&gt;\([0,255]\)&lt;/span&gt;. A single image can be thought of as a point in a 268,203-dimensional vector space. An ImageNet classifier is mapping that input space to a space of 1000 probabilities, one for each class. We train the classifier with training data to be a function that takes in any point in the input space and outputs a point in the output space. Given an image, here are the class probabilities.&lt;/p&gt;
&lt;p&gt;Our training data is comprised of 1,000,000 natural images from the 1,000 classes. That is a lot of data and that is what makes training these neural networks possible. However, we know that this is actually only a tiny subset of all possible data. Imagine all possible photos of all possible instances of all possible ImageNet classes taken from all possible angles under all possible lighting conditions, and so on and so on. The number of possible images is clearly much much greater than the one million images in our training data. It would be tempting to conclude that the number is infinite, but we know that is actually not the case. Consider drawing randomly from the space of all possible 299x299x3 images. Set every pixel value randomly and study the result. How long would you expect to draw random images until you got one that looks like a normal photograph? I think probably all of humanity could perform this task for the rest of time without being successful.&lt;/p&gt;
&lt;div class="math"&gt;$$ N_{training} &amp;lt;&amp;lt; N_{natural} &amp;lt;&amp;lt; {256}^{260203} $$&lt;/div&gt;
&lt;p&gt;So we are left with the conclusions that natural images make up a tiny proportion of our input vector space and that our training data makes up a tiny proportion of natural images. That means our classifier is learning to map a vector space that is mostly noisy garbage images, and it's learning to do that from actually very limited training data.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1&gt;Where do adversarial images come from?&lt;/h1&gt;
&lt;p&gt;To build an intuition with visualization, we will consider an simplified two-dimensional example. No longer will we talk about 299 pixels by 299 pixels RGB images that define 260,203-dimensional vector spaces. Let's instead pretend that we have 2 pixels by 1 pixel gray scale images. They have only two pixels and only one value per pixel. Somehow imagine that it is possible to identify some of these tiny images as cats and some as dogs. We have six images in our training set (3 cats, 3 dogs), and since there are only two values, we can visualize them in a scatter plot. We will seek to train a classifier that can take in another of these strange images, and output a class of cat or dog.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
Our two pixel images in a scatter plot:
&lt;center&gt;
&lt;img alt="Two Dimensional Example" src="/images/where_do_adversarial/1-two-dims.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
Our training set is six images. Three cats and three dogs. Each image has a different value for pixel 1 and for pixel 2. We are therefore able to plot these images in our scatter plot.
&lt;center&gt;
&lt;img alt="Training Data" src="/images/where_do_adversarial/3-training-data.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
There is a true distribution of images that our training data comes from. It is impossible for us to ever know it, because we only have our training data points. Some regions are cats, some regions are dogs, and some regions are neither, perhaps just random meaningless images. &lt;strong&gt;I'll make up two hypothetical regions.&lt;/strong&gt;
&lt;center&gt;
&lt;img alt="Unknowable Truth" src="/images/where_do_adversarial/2-unknowable-truth.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
We use the training examples to train the classifier to draw a decision boundary between them. The boundary is a line where the probability of cat and dog are equal at 0.5, and on either side the decision is clear. We hope that when new test examples are drawn from somewhere in the red or blue areas, they will be classified correctly.&lt;/p&gt;
&lt;p&gt;Unfortunately the training examples don't cover the space entirely, so there are a lot of different boundaries that could be drawn. This will always lead to errors at test time.
&lt;center&gt;
&lt;img alt="Decision Boundaries" src="/images/where_do_adversarial/4-decision-boundaries.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
There are large areas with no training data that we don't care about at all. The neural network can do whatever it wants with the decision boundaries in this space. It's chaos for all we know.
&lt;center&gt;
&lt;img alt="Chaos" src="/images/where_do_adversarial/5-chaos.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
Consider the following decision boundary. It looks a little strange, but it correctly classifies the training data, does a decent job of covering the true space, and behaves weirdly in the empty space. It seems strange in our two dimensional case, but for larger images this is very plausible.
&lt;center&gt;
&lt;img alt="Plausible" src="/images/where_do_adversarial/6-plausible.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
In the diagram below I use a black arrow to indicate an attack. We take a natural image from our training data, and then add or subtract a little from each of the two pixel values in order to move it into a different area of the scatter plot.&lt;/p&gt;
&lt;p&gt;This gives us three ways of creating adversarial images by making small changes.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;We move a cat from the region of true cats that were correctly classified, into a region where they are not.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We move a dog out of the region of natural images, and into the area where the decision boundaries are chaotic. We choose a place that is classified as cat.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We move a cat from the region of true cats into the region of true dogs, causing it to be misclassified. In theory this attack would work on a human because we literally changed the meaning of the image.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="Attacks" src="/images/where_do_adversarial/7-attacks.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;If the change is small enough, a human won't notice the difference.&lt;/p&gt;
&lt;p&gt;When attackers are creating these adversarial images, they don't really know which of the three scenarios are occurring. If the attackers could know that, then so could the defenders and adversarial images wouldn't be a problem.&lt;/p&gt;
&lt;p&gt;At the top of the article we had a panda changed into a gibbon. I think that we can agree this is an example of #1 above. The image is still a normal looking image of a panda, but it is misclassified.&lt;/p&gt;
&lt;p&gt;Here's an interesting example of a natural image and an attacked image from &lt;a href="https://www.kaggle.com/benhamner"&gt;Ben Hamner&lt;/a&gt;'s &lt;a href="https://www.kaggle.com/benhamner/adversarial-learning-challenges-getting-started"&gt;kernel&lt;/a&gt;:
&lt;center&gt;
&lt;img alt="FGSM" src="/images/where_do_adversarial/fgsm.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;The attacked image on the right is still obviously a butterfly, but there's something odd about it. There's a bizarre noise pattern that would not occur in a normal image. This is an example of #2.&lt;/p&gt;
&lt;p&gt;Finally here's something I created recently as part of a project:
&lt;center&gt;
&lt;img alt="Natural" src="/images/where_do_adversarial/2ba52bcf59097dc8_natural.png"&gt; &lt;img alt="Attacked" src="/images/where_do_adversarial/2ba52bcf59097dc8.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;The goal was to get an ImageNet classifier to output something other than mountain, which it obviously is. The attacked mountain on the right has an unnatural quality to it, but to a human eye, it actually looks quite a lot like a jigsaw puzzle of a mountain, as was my intent. This is an example of #3. We have changed the true meaning of the image such that even a human might get it wrong.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1&gt;What can we do to defend?&lt;/h1&gt;
&lt;p&gt;One of the most basic defenses is "adversarial training". Essentially we attack our own classifier and then add those examples as training data. Doing so allows us to pad the area around our training data so that decision boundaries are well behaved there, even though no "natural image" should ever appear there. This is effective to some extent, but with hundred of thousands of dimensions to cover, it seems hopeless that we'll ever be able to pad the entire space of natural images, let alone the space around our training data.&lt;/p&gt;
&lt;p&gt;We add the examples from our three attacks above and re-train our classifier to get a new decision boundary.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="Adversarial Training" src="/images/where_do_adversarial/8-adversarial-training.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
As an aside, this is very similar to what we are doing when we do data augmentation. Using our prior knowledge about what transforms do not change the meaning of an image, we create more training data in the region of each image to help the classifier map the space.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="Data Augmentation" src="/images/where_do_adversarial/9-augmentation.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;So there you go. I hope this gives you an intuition for adversarial images and makes them a little less magical.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>Ontario Sunshine List Pathologists Notebook</title><link href="/gist-pathologists.html" rel="alternate"></link><published>2015-03-24T20:00:00+00:00</published><updated>2015-03-24T20:00:00+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2015-03-24:/gist-pathologists.html</id><summary type="html">&lt;p&gt;Here is an &lt;a href="http://nbviewer.ipython.org/gist/alekseynp/81a249e5e971750039dc"&gt;nbviewer IPython notebook showing an analysis of pathologists in the Ontario Sunshine List&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It uses data from the &lt;a href="https://github.com/alekseynp/ontario_sunshine_list"&gt;Ontario Sunshine List Scraper&lt;/a&gt; that I made public on GitHub recently.&lt;/p&gt;
&lt;p&gt;It recreates the analysis that went in to my previous article about pathologists on the list: &lt;a href="http://alekseynp.com/2013/04/19/20-25-raise-for-ontarios-pathologists-in-201-shows-sunshine-list/"&gt;20-25% Raise …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here is an &lt;a href="http://nbviewer.ipython.org/gist/alekseynp/81a249e5e971750039dc"&gt;nbviewer IPython notebook showing an analysis of pathologists in the Ontario Sunshine List&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It uses data from the &lt;a href="https://github.com/alekseynp/ontario_sunshine_list"&gt;Ontario Sunshine List Scraper&lt;/a&gt; that I made public on GitHub recently.&lt;/p&gt;
&lt;p&gt;It recreates the analysis that went in to my previous article about pathologists on the list: &lt;a href="http://alekseynp.com/2013/04/19/20-25-raise-for-ontarios-pathologists-in-201-shows-sunshine-list/"&gt;20-25% Raise for Ontario’s Pathologists in 2012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I hope you'll find it useful.&lt;/p&gt;</content></entry><entry><title>Ontario Sunshine List Open Scraper</title><link href="/ontario-sunshine-list-scraper.html" rel="alternate"></link><published>2015-03-07T16:30:00+00:00</published><updated>2015-03-07T16:30:00+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2015-03-07:/ontario-sunshine-list-scraper.html</id><summary type="html">&lt;p&gt;Today I am announcing my open Ontario Sunshine List Scraper released under the MIT License.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You can download the data directly &lt;a href="https://github.com/alekseynp/ontario_sunshine_list/blob/master/output/data.csv"&gt;here&lt;/a&gt;.&lt;/strong&gt; I will likely move this later.&lt;/p&gt;
&lt;p&gt;Anyone can go &lt;a href="https://github.com/alekseynp/ontario_sunshine_list"&gt;here to the GitHub repo&lt;/a&gt; and download python code that scrapes the &lt;a href="http://www.fin.gov.on.ca/en/publications/salarydisclosure/pssd/"&gt;Ontario Public Sector Salary Disclosure&lt;/a&gt; data into …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today I am announcing my open Ontario Sunshine List Scraper released under the MIT License.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You can download the data directly &lt;a href="https://github.com/alekseynp/ontario_sunshine_list/blob/master/output/data.csv"&gt;here&lt;/a&gt;.&lt;/strong&gt; I will likely move this later.&lt;/p&gt;
&lt;p&gt;Anyone can go &lt;a href="https://github.com/alekseynp/ontario_sunshine_list"&gt;here to the GitHub repo&lt;/a&gt; and download python code that scrapes the &lt;a href="http://www.fin.gov.on.ca/en/publications/salarydisclosure/pssd/"&gt;Ontario Public Sector Salary Disclosure&lt;/a&gt; data into a machine-readable format.&lt;/p&gt;
&lt;p&gt;Today's version of the code has two key limitations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Only the initial disclosure is scraped. Addenda are not scraped or processed.&lt;/li&gt;
&lt;li&gt;2015 disclosure has not yet been published or included&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please feel free to fork the repo and build ahead.&lt;/p&gt;
&lt;p&gt;Anyone can create their own CSV like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;ontario_sunshine_list&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;osl&lt;/span&gt;

&lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;osl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Collector&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/home/aleksey/data/sunshine/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;scr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;osl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Scraper&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/home/aleksey/data/sunshine/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;cle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;osl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Cleaner&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/home/aleksey/data.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content></entry><entry><title>Gephi Layout Plugin: Random 3D Layout</title><link href="/gephi-layout-plugin-random-3d-layout.html" rel="alternate"></link><published>2014-05-15T18:02:45+00:00</published><updated>2014-05-15T18:02:45+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2014-05-15:/gephi-layout-plugin-random-3d-layout.html</id><summary type="html">&lt;p&gt;&lt;img alt="bigRandomCube" src="/images/bigRandomCube-300x266.png"&gt;Yesterday, I released a simple plugin, &lt;a href="https://marketplace.gephi.org/plugin/random-3d-layout/"&gt;Random 3D Layout t&lt;/a&gt;o the  Gephi Marketplace &lt;a href="https://marketplace.gephi.org/plugin/random-3d-layout/"&gt;here&lt;/a&gt;. It's very simple, and I developed it as part of my work at &lt;a href="http://www.ngrain.com"&gt;NGRAIN&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For anyone working in 3D in Gephi, this will be a useful and simple initialization layout plugin. Works well with the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="bigRandomCube" src="/images/bigRandomCube-300x266.png"&gt;Yesterday, I released a simple plugin, &lt;a href="https://marketplace.gephi.org/plugin/random-3d-layout/"&gt;Random 3D Layout t&lt;/a&gt;o the  Gephi Marketplace &lt;a href="https://marketplace.gephi.org/plugin/random-3d-layout/"&gt;here&lt;/a&gt;. It's very simple, and I developed it as part of my work at &lt;a href="http://www.ngrain.com"&gt;NGRAIN&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For anyone working in 3D in Gephi, this will be a useful and simple initialization layout plugin. Works well with the &lt;a href="https://marketplace.gephi.org/plugin/force-atlas-3d/"&gt;Force Atlas 3D Plugin&lt;/a&gt;. Without it and using a 2D layout to initialize instead, the 3D results can sometimes be milky-way-shaped, not generally spherical as you would expect, and ultimately not taking full advantage of the third dimension.&lt;/p&gt;</content></entry><entry><title>My Ontario Sunshine List work in the CBC News</title><link href="/my-ontario-sunshine-list-work-in-the-cbc-news.html" rel="alternate"></link><published>2014-04-01T16:13:29+00:00</published><updated>2014-04-01T16:13:29+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2014-04-01:/my-ontario-sunshine-list-work-in-the-cbc-news.html</id><summary type="html">&lt;p&gt;Just some shameless self-promotion. You can find my work cited in the CBC News from today.&lt;/p&gt;
&lt;p&gt;Kazi Stastna writes &lt;a href="http://www.cbc.ca/news/canada/sunshine-list-2014-ontario-s-list-drives-salaries-up-not-down-1.2592793"&gt;Sunshine List 2014: Ontario's list drives salaries up, not down&lt;/a&gt; and includes:&lt;/p&gt;
&lt;blockquote&gt;Ontario pathologists did just that and saw their Sunshine List salaries increase by 20 to 25 per cent between …&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;Just some shameless self-promotion. You can find my work cited in the CBC News from today.&lt;/p&gt;
&lt;p&gt;Kazi Stastna writes &lt;a href="http://www.cbc.ca/news/canada/sunshine-list-2014-ontario-s-list-drives-salaries-up-not-down-1.2592793"&gt;Sunshine List 2014: Ontario's list drives salaries up, not down&lt;/a&gt; and includes:&lt;/p&gt;
&lt;blockquote&gt;Ontario pathologists did just that and saw their Sunshine List salaries increase by 20 to 25 per cent between 2011 and 2012, compared with the 2.2 per cent average for the list as a whole, according to an [analysis](http://alekseynp.com/2013/04/19/20-25-raise-for-ontarios-pathologists-in-201-shows-sunshine-list/) done by data blogger Aleksey Nozdryn-Plotnicki.

...

Nozdryn-Plotnicki [found](http://alekseynp.com/2013/04/19/7-2-raise-for-1000-best-paid-ontario-public-sector-employees/) that this inflationary effect is greatest at the upper echelons of the Sunshine List, with the salaries of the 1,000 highest-paid workers rising 7.2 per cent between 2011 and 2012, compared with 2.2 per cent for the bottom half of the list.&lt;/blockquote&gt;

&lt;p&gt;They are, of course, referring to my work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://alekseynp.com/2013/04/19/7-2-raise-for-1000-best-paid-ontario-public-sector-employees/"&gt;7.2% raise for 1,000 best paid Ontario public sector employees&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://alekseynp.com/2013/04/19/20-25-raise-for-ontarios-pathologists-in-201-shows-sunshine-list/"&gt;20-25% raise for Ontario’s pathologists in 2012
&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For those outside of Canada, the CBC, the Canadian Broadcasting Corporation is a public entity and major player in the Canadian news media market. It's my personal number one source for Canadian news.&lt;/p&gt;</content></entry><entry><title>I tried to sell open government data</title><link href="/i-tried-to-sell-open-government-data.html" rel="alternate"></link><published>2014-03-29T02:13:28+00:00</published><updated>2014-03-29T02:13:28+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2014-03-29:/i-tried-to-sell-open-government-data.html</id><summary type="html">&lt;p&gt;I tried to sell open government data, but I won't be the only one.&lt;/p&gt;
&lt;p&gt;In 2013 I wrote a series of data journalism articles on the topic of &lt;a href="http://alekseynp.com/topic-ontario-public-salary-disclosure/"&gt;Ontario Public Sector Salary Disclosure&lt;/a&gt;. The posts were based on data made available as part of a disclosure initiative in the Canadian …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I tried to sell open government data, but I won't be the only one.&lt;/p&gt;
&lt;p&gt;In 2013 I wrote a series of data journalism articles on the topic of &lt;a href="http://alekseynp.com/topic-ontario-public-salary-disclosure/"&gt;Ontario Public Sector Salary Disclosure&lt;/a&gt;. The posts were based on data made available as part of a disclosure initiative in the Canadian province of Ontario. Data was published freely and openly on the Ontario Ministry of Finance website in html and pdf format. I performed a convoluted scraping and processing exercise to build a definitive, unified data set and then performed some analysis in order to identify some interesting headlines.&lt;/p&gt;
&lt;p&gt;In late 2013 I was approached by someone who was looking for the data behind my analysis. I agreed to share the data on a few conditions, one of which was non-commercial use that they were unable to meet. And so I offered to sell them the data. Free, open, public data.&lt;/p&gt;
&lt;blockquote&gt;Sorry Aleksey. Not going to pay for public data even if private hours went into scraping it.&lt;/blockquote&gt;

&lt;p&gt;I knew I had entered a moral landscape with shades of grey on a topic that people would be emotional about.&lt;/p&gt;
&lt;p&gt;Government data should be free and open! After all it's our data for us! Well except perhaps in instances where user fees would be a more appropriate measure to spread the cost of data production more proportionally to those who will benefit. And the shades are grey.&lt;/p&gt;
&lt;p&gt;My simplest and most direct defense is: My customer knew that the data was freely available, but I still had something to sell them.&lt;/p&gt;
&lt;p&gt;Nobody owns the truth, but once they have measured it, processed it, and made it available in a useful format, then they have something that they could sell to you. If I sold you a map based on free government data or delivered you a service based on it, you would accept that.&lt;/p&gt;
&lt;p&gt;I wrote about the &lt;a href="http://alekseynp.com/2013/05/27/data-sins-of-the-ontario-ministry-of-finance-public-sector-salary-disclosure/"&gt;Data Sins of the Ontario Ministry of Finance – Public Sector Salary Disclosure&lt;/a&gt; and &lt;a href="http://alekseynp.com/2013/04/22/current-publishing-of-ontario-sunshine-list-not-good-enough/"&gt;Current publishing of Ontario “Sunshine List” not good enough&lt;/a&gt;. This data was "open" but not truly "open" and it required considerable effort to process before I could analyse it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ProPublica&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Why I wrote this today rather than continue to procrastinate is that I saw this: &lt;a href="http://www.propublica.org/article/introducing-the-propublica-data-store"&gt;Introducing the ProPublica Data Store&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;For datasets that are the result of significant expenditures of our time and effort, we're charging a reasonable one-time fee: In most cases, it's $200 for journalists and $2,000 for academic researchers. Those wanting to use data commercially should reach out to us to discuss pricing.&lt;/blockquote&gt;

&lt;p&gt;And relevantly, they have data sets for sale based on data acquired through the American Freedom of Information Act.&lt;/p&gt;</content></entry><entry><title>Analysis of the determinants of selling price for Vancouver NHL ticket auctions</title><link href="/analysis-of-the-determinants-of-selling-price-for-vancouver-nhl-ticket-auctions.html" rel="alternate"></link><published>2014-02-16T01:11:10+00:00</published><updated>2014-02-16T01:11:10+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2014-02-16:/analysis-of-the-determinants-of-selling-price-for-vancouver-nhl-ticket-auctions.html</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I've been scraping a lot lately. If data is the new oil, then it's time I started pumping it. I came across this old project of mine from a statistics course in my masters program, dated December 15, 2006. Stubhub today should be a goldmine of data and I …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I've been scraping a lot lately. If data is the new oil, then it's time I started pumping it. I came across this old project of mine from a statistics course in my masters program, dated December 15, 2006. Stubhub today should be a goldmine of data and I hope appropriate data and/or analysis is being passed to the leagues to help them set prices.&lt;/p&gt;
&lt;blockquote&gt;**Analysis of the determinants of selling price for Vancouver NHL ticket auctions**

Data was collected for a one month period of auctions completed on eBay, a popular internet auction site, for tickets to see the Vancouver Canucks play at home.  The data was analyzed to determine a good regression model for the selling price per ticket. The most significant indicator was face value, but auctions by sellers with higher feedback scores and transactions that are completed longer before a game also finished with a higher price per ticket. Noteworthy insignificant factors were number of tickets in a lot, the feedback percentage rating for the seller, and the length of the auction.&lt;/blockquote&gt;

&lt;h1&gt;Findings&lt;/h1&gt;
&lt;p&gt;My most interesting finding would provide guidance to price setting.&lt;/p&gt;
&lt;blockquote&gt;Of most interest is the regular game Upper Bowl IV and Upper Bowl V price categories. The eBay market price difference from Ticketmaster is significant and in the case of Upper Bowl V seats, sizable. This indicates that these tickets are priced well below market demand. It would seem that from the consumer’s perspective Upper Bowl IV and V are indistinguishable and the Canucks should seriously consider charging more for these seats.&lt;/blockquote&gt;

&lt;p&gt;I also found that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Tickets sold further ahead of game day sold for more. There is a fundamental tension in the market for tickets, a time-valued good (after game day a ticket is worth zero). Some shoppers are will pass on an auction hoping to find a better deal later. Some shoppers are willing to pay more for the certainty of having a ticket now. Some sellers want to unload their tickets now before they become worthless. Some sellers will patiently wait out the buyers in order to fetch a higher price. In my data there was a correlation of 0.3 between minimum bid and time until game day. This is evidence  of patient sellers setting and getting higher prices further ahead of game day.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Feedback rating was a more important factor than feedback percentage. In the marketplace that I studied where most operators were small-scale with 100% positive feedback, their tickets sold at a higher price if they gathered more positive feedback. On the flip-side, big-time operators with only a little less than 100% feedback did not suffer greatly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The number of tickets in a lot did not affect selling price per ticket. You would expect four tickets together to be worth more than two separate pairs, and surely they are, but there was not enough evidence in the data, so it is not a strong impact.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Length of the auction did not have an effect. Longer auctions presumably get seen by more people and therefore sell for more, unless the market is active enough such that a critical mass of buyers will see auctions of short length.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;h1&gt;How I Did It&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A scraper was written in C# to collect html pages from eBay of completed auctions for Vancouver Canucks tickets&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extensive regular expression matching was done on the auction's free text in order to determine what game the tickets were for, how many tickets there were, and other factors like where the seats were in the stadium&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Regression models were tried in order to find a good fit&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Regression coefficients were interpreted to draw conclusions&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content></entry><entry><title>Average shot locations in the paint and at midrange</title><link href="/average-shot-locations-in-the-paint-and-at-midrange.html" rel="alternate"></link><published>2013-11-08T00:07:15+00:00</published><updated>2013-11-08T00:07:15+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-11-08:/average-shot-locations-in-the-paint-and-at-midrange.html</id><summary type="html">&lt;p&gt;Yet another post on visualising and analysing NBA shot location data using location averaging methods.&lt;/p&gt;
&lt;p&gt;Previously I have shown averages by team for all shots taken. What about shots taken by zone? Consider the following two charts:&lt;/p&gt;
&lt;p&gt;&lt;img alt="avg_inthepaint_teams" src="/images/avg_inthepaint_teams.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="avg_midrange_teams" src="/images/avg_midrange_teams.png"&gt;&lt;/p&gt;
&lt;p&gt;Now we can see a level of detail that we couldn't in the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yet another post on visualising and analysing NBA shot location data using location averaging methods.&lt;/p&gt;
&lt;p&gt;Previously I have shown averages by team for all shots taken. What about shots taken by zone? Consider the following two charts:&lt;/p&gt;
&lt;p&gt;&lt;img alt="avg_inthepaint_teams" src="/images/avg_inthepaint_teams.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="avg_midrange_teams" src="/images/avg_midrange_teams.png"&gt;&lt;/p&gt;
&lt;p&gt;Now we can see a level of detail that we couldn't in the average of all shots.&lt;/p&gt;
&lt;p&gt;Previously:&lt;/p&gt;
&lt;p&gt;&lt;a href="more-on-averages-shot-locations.html"&gt;&lt;img alt="for_and_against_640w" src="/images/for_and_against_640w-150x150.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Previously we saw that the GSW were the longest shooters in the league and indeed they were also long shooters in the paint and at midrange. It is not simply that the GSW take a lot of 3s.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Other teams like the NYK take shots from close up in both the paint and at midrange. They were generally longer shooters in the previous analysis, suggesting that they balance those close 2-pts with many 3-pt attempts&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Previously we saw DEN as the closest shooters overall, and indeed they appear to be close shooters in the paint and moderately close shooters at midrange.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>NBA players shoot from further as the game progresses</title><link href="/nba-players-shoot-from-further-as-the-game-progresses.html" rel="alternate"></link><published>2013-11-07T21:02:15+00:00</published><updated>2013-11-07T21:02:15+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-11-07:/nba-players-shoot-from-further-as-the-game-progresses.html</id><summary type="html">&lt;p&gt;On average, players in the NBA take shots 6.5% further from the basket in the 4th period than in the 1st. This is a subtle, but consistent trend across all periods. 1.2% further in the 2nd than the 1st, 2.4% further in the 3rd than the 2nd …&lt;/p&gt;</summary><content type="html">&lt;p&gt;On average, players in the NBA take shots 6.5% further from the basket in the 4th period than in the 1st. This is a subtle, but consistent trend across all periods. 1.2% further in the 2nd than the 1st, 2.4% further in the 3rd than the 2nd, and 2.7% further in the 4th than the 3rd.&lt;/p&gt;
&lt;p&gt;Most teams show the same trend, consider the graphic Average Distance of Shots by Period below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="avg_distance_period_allteams" src="/images/avg_distance_period_allteams.png"&gt;&lt;/p&gt;
&lt;p&gt;Nearly all teams show an increasing distance by period, but there are some notable exceptions. A few teams like NYK, UTA, ATL, and ORL show an opposite trend.&lt;/p&gt;
&lt;p&gt;It's not clear at this point what is leading this. Greater defense in later periods forces further shots? A greater need for 3-pts pushes shots away from the basket? A greater need for points in less time forces less ideal shots from further out? What about the exceptional teams where they get closer? What are these teams doing in the 4th quarter? Leaning more heavily on particular strengths or players?  Tired players not driving to the net?&lt;/p&gt;
&lt;p&gt;What is true at the global level and largely true at the team level is again reflected in the players. Here are the average distance for the top 100 scorers:&lt;/p&gt;
&lt;p&gt;&lt;img alt="avg_distance_period_100players" src="/images/avg_distance_period_100players.png"&gt;&lt;/p&gt;
&lt;p&gt;More noise, but the trend is still visible!&lt;/p&gt;</content></entry><entry><title>More on averages shot locations</title><link href="/more-on-averages-shot-locations.html" rel="alternate"></link><published>2013-11-05T15:58:08+00:00</published><updated>2013-11-05T15:58:08+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-11-05:/more-on-averages-shot-locations.html</id><summary type="html">&lt;p&gt;Here are some more visuals based on my analysis of average shot locations from the NBA Regular Season 2012-13...&lt;/p&gt;
&lt;p&gt;See previously:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="average-made-shot-location-nba-regular-season-2012-13.html"&gt;Average Made Shot Location - NBA Regular Season 2012-13&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="update-average-shot-locations-nba-regular-season-2012-13.html"&gt;Update: Average shot locations - NBA Regular Season 2012-13&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Average Location of Shots Made - NBA 2012-13 Regular Season by teams and against …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here are some more visuals based on my analysis of average shot locations from the NBA Regular Season 2012-13...&lt;/p&gt;
&lt;p&gt;See previously:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="average-made-shot-location-nba-regular-season-2012-13.html"&gt;Average Made Shot Location - NBA Regular Season 2012-13&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="update-average-shot-locations-nba-regular-season-2012-13.html"&gt;Update: Average shot locations - NBA Regular Season 2012-13&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Average Location of Shots Made - NBA 2012-13 Regular Season by teams and against teams...&lt;/p&gt;
&lt;p&gt;&lt;img alt="for_and_against_640w" src="/images/for_and_against_640w1.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Shots made by a team show more variation than shots made against a team. Where you shoot from depends more on who you are than who you are playing against -- fine.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A lot more front-back variation than side-to-side, probably to be expected&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Shots made by MIN, DET, SAS close to the net and DAL, GSW, NYK far away somewhat interesting. Fairly big difference here actually which can probably be chalked up to specific players&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Shots made &lt;em&gt;against&lt;/em&gt; probably more interesting. MIL way down at the bottom, GSW significantly at the top. What about these teams' defensive style means they are more inclined to allow shots close or far from the net?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sketch (not poblished):&lt;/p&gt;
&lt;p&gt;Average Shot Location by Team by Win/Loss - NBA Regular Season 2012-13&lt;/p&gt;
&lt;p&gt;&lt;a href="/images/sketch_wins_losses_byteam_v22.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Every line represents a team and connects the two points for that team. The white dot for a team is the average location of shots attempted in games that team won. The black dot for a team is the average location of shots attempted in games that team lost.&lt;/p&gt;
&lt;p&gt;Analysis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Most wins and losses points are near one another&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lines generally do not cross&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are a number of teams across the middle of the chart who attempt shots closer to the net more often when they win&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Variation is left/right variation is probably just statistical noise&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Same chart but filtering for only successful shots... same story, though less evidence that closer shots are important for winning:&lt;/p&gt;
&lt;p&gt;&lt;img alt="sketch_wins_losses_byteam_made_v2" src="/images/sketch_wins_losses_byteam_made_v2.png"&gt;&lt;/p&gt;
&lt;p&gt;We could also look at the games for an individual team... and I think learn nothing:&lt;/p&gt;
&lt;p&gt;&lt;img alt="1610612737" src="/images/1610612737.png"&gt;&lt;/p&gt;</content></entry><entry><title>Update: Average shot locations - NBA Regular Season 2012-13</title><link href="/update-average-shot-locations-nba-regular-season-2012-13.html" rel="alternate"></link><published>2013-10-24T21:45:40+00:00</published><updated>2013-10-24T21:45:40+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-10-24:/update-average-shot-locations-nba-regular-season-2012-13.html</id><summary type="html">&lt;p&gt;Here are two updated charts following up from my&lt;a href="average-made-shot-location-nba-regular-season-2012-13.html"&gt; previous post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="wgtavg_made_location_top50scorers_201213_regular" src="/images/wgtavg_made_location_top50scorers_201213_regular.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="avg_made_location_top50scorers_201213_regular" src="/images/avg_made_location_top50scorers_201213_regular.png"&gt;&lt;/p&gt;
&lt;p&gt;What's new?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The players included are no longer someone else' list of 50 notable players, but rather the 50 players who scored the most points in the 2012-13 regular season&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;More of the courts lines have been included to …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Here are two updated charts following up from my&lt;a href="average-made-shot-location-nba-regular-season-2012-13.html"&gt; previous post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="wgtavg_made_location_top50scorers_201213_regular" src="/images/wgtavg_made_location_top50scorers_201213_regular.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="avg_made_location_top50scorers_201213_regular" src="/images/avg_made_location_top50scorers_201213_regular.png"&gt;&lt;/p&gt;
&lt;p&gt;What's new?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The players included are no longer someone else' list of 50 notable players, but rather the 50 players who scored the most points in the 2012-13 regular season&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;More of the courts lines have been included to provide greater context to the data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The first chart shows the average position that points are scored from. This is a weighted average that puts more emphasis on 3 pointers and further demonstrates how exceptional Stephen Curry is, as the guys at &lt;a href="http://regressing.deadspin.com/stephen-currys-numbers-are-as-unlikely-and-wonderful-a-1449057524"&gt;deadspin have noted&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second chart shows the average position of made shots as per the original&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Collaboration&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Collaboration is an important thing. Earlier with my weak knowledge of basketball I published what I thought would be an interesting visual &lt;a href="average-made-shot-location-nba-regular-season-2012-13.html"&gt;here&lt;/a&gt;. Seems I was correct. But with a little conversation with the guys at &lt;a href="deadspin.com"&gt;deadspin.com&lt;/a&gt;, some very easy improvements could be made.&lt;/p&gt;</content></entry><entry><title>Visualising Shot Location in Basketball</title><link href="/visualising-shot-location-in-basketball.html" rel="alternate"></link><published>2013-10-14T19:30:30+00:00</published><updated>2013-10-14T19:30:30+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-10-14:/visualising-shot-location-in-basketball.html</id><summary type="html">&lt;p&gt;Sports is full of spacial data. Data visualizers love this. Take the points and plot them by x and y location. Instantly the data is in a format that we can relate to and gain insight from. Too many data points? Aggregate and count points by region to create a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Sports is full of spacial data. Data visualizers love this. Take the points and plot them by x and y location. Instantly the data is in a format that we can relate to and gain insight from. Too many data points? Aggregate and count points by region to create a head map or something similar. Want to compare two sets of points? Put two charts side-by-side. All of this is done with NBA shot position, but it has its shortcomings.&lt;/p&gt;
&lt;p&gt;At stats.nba.com you can see shot charts like this:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://stats.nba.com/playerShotchart.html?PlayerID=201142&amp;amp;viewShots=true&amp;amp;zoneDetails=false&amp;amp;zoneOverlays=false"&gt;&lt;img alt="short_chart_thumbnail" src="/images/short_chart_thumbnail.png"&gt;&lt;/a&gt; &lt;a href="http://stats.nba.com/playerShotchart.html?PlayerID=201142&amp;amp;viewShots=false&amp;amp;zoneDetails=true&amp;amp;zoneOverlays=true&amp;amp;zone-mode=zone"&gt;&lt;img alt="shot_chart_thumbnail2" src="/images/shot_chart_thumbnail2.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;At hotshotcharts.com you can see shot charts heated by success rate:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://hotshotcharts.com/"&gt;&lt;img alt="shot_chart_thumbnail3" src="/images/shot_chart_thumbnail3.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Even at the much (rightly) worshiped New York Times they're doing charts like this:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.nytimes.com/interactive/2012/06/11/sports/basketball/nba-shot-analysis.html?_r=0"&gt;&lt;img alt="shot_chart_thumbnail4" src="/images/shot_chart_thumbnail4.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shotcomings&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I would say that the number one shortcoming of these charts is the difficultly in comparing two sets of points. Take the New York Times piece for example. Try to compare the Heat to the Thunder or the various pairs of players side-by-side. Your eyes will flick back and forth and maybe you'll find a difference, but it's hard and you can't help but feel that you are missing something.&lt;/p&gt;
&lt;p&gt;Why? Too much data. The points are not together for easy comparison.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What I am experimenting with is greater use of aggregation in these circumstances. What if we take average positions to aggregate thousands of data points and then plot those averages together to gain insights?&lt;/p&gt;
&lt;p&gt;Here is &lt;a href="average-made-shot-location-nba-regular-season-2012-13.html"&gt;my first piece&lt;/a&gt; on basketball:&lt;/p&gt;
&lt;p&gt;&lt;a href="/images/final.png"&gt;&lt;img alt="final" src="/images/final-298x300.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately the concept of average location is a bit abstract, and thus that creates a greater overhead of effort for the viewer to overcome before they start getting value out of the graphic. However, after that we can start to compare players in a way that we never have before. See the first post &lt;a href="http://alekseynp.com/2013/10/13/average-made-shot-location-nba-regular-season-2012-13/"&gt;here&lt;/a&gt;.&lt;/p&gt;</content></entry><entry><title>Average Made Shot Location - NBA Regular Season 2012-13</title><link href="/average-made-shot-location-nba-regular-season-2012-13.html" rel="alternate"></link><published>2013-10-13T19:14:35+00:00</published><updated>2013-10-13T19:14:35+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-10-13:/average-made-shot-location-nba-regular-season-2012-13.html</id><summary type="html">&lt;p&gt;&lt;img alt="final" src="/images/final.png"&gt;&lt;/p&gt;
&lt;p&gt;In the 2012-13 regular season, tens of thousands of shots were taken. How can we visualize these in a useful manner? What is the average location of successful shot attempts? How do the top 50 players compare?&lt;/p&gt;
&lt;p&gt;Note: In terms of magnitude, the points above fall roughly within the key …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="final" src="/images/final.png"&gt;&lt;/p&gt;
&lt;p&gt;In the 2012-13 regular season, tens of thousands of shots were taken. How can we visualize these in a useful manner? What is the average location of successful shot attempts? How do the top 50 players compare?&lt;/p&gt;
&lt;p&gt;Note: In terms of magnitude, the points above fall roughly within the key.&lt;/p&gt;
&lt;p&gt;Interesting observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Outliers or rather, those players who are the most different:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Omer Asik and Dwight Howard very close to the basket&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stephen Curry and Jarrett Jack very far from the basket&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kevin Martin way out to the left&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Right-hand bias. The majority of players are making shots on average from the right-hand side of the court&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Trend. Notice that the dots seem to form a line.  The majority of the players are on the right-hand side and as you move further from the basket, they also move further right. This may reveal something fundamental about the game, that there is some sort of ideal-angle to be shooting from. Note that the points on the left-hand side may have a similar trend, but it's not as strong.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Know something about basketball? What insights do you get from this chart?&lt;/p&gt;
&lt;p&gt;More about this from a data visualization perspective in another post.&lt;/p&gt;
&lt;p&gt;See also my work on the average passing direction in the English Premier League (Football (Soccer)):&lt;/p&gt;
&lt;p&gt;&lt;a href="viz/mcfc-opta-passing-permier-league-2011-12.html"&gt;&lt;img alt="passingDirection" src="/images/passingDirection1.png"&gt;&lt;/a&gt;&lt;/p&gt;</content></entry><entry><title>Average age of Brooklyn's buildings mapped</title><link href="/average-age-of-brooklyns-buildings-mapped.html" rel="alternate"></link><published>2013-08-17T04:12:25+00:00</published><updated>2013-08-17T04:12:25+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-08-17:/average-age-of-brooklyns-buildings-mapped.html</id><summary type="html">&lt;p&gt;I saw &lt;a href="http://bklynr.com/block-by-block-brooklyns-past-and-present/"&gt;this&lt;/a&gt; recently &lt;a href="http://www.theatlanticcities.com/neighborhoods/2013/08/interactive-map-day-brooklyns-buildings-are-newer-you-think/6399/"&gt;here&lt;/a&gt;. It's a map showing the age of Brooklyn's buildings.&lt;/p&gt;
&lt;p&gt;&lt;img alt="bklynr" src="/images/bklynr.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What do I get from the map?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Every data point is plotted, so if I were a Brooklyner, I would probably find my street or look up my favourite building. For some people this seems to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I saw &lt;a href="http://bklynr.com/block-by-block-brooklyns-past-and-present/"&gt;this&lt;/a&gt; recently &lt;a href="http://www.theatlanticcities.com/neighborhoods/2013/08/interactive-map-day-brooklyns-buildings-are-newer-you-think/6399/"&gt;here&lt;/a&gt;. It's a map showing the age of Brooklyn's buildings.&lt;/p&gt;
&lt;p&gt;&lt;img alt="bklynr" src="/images/bklynr.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What do I get from the map?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Every data point is plotted, so if I were a Brooklyner, I would probably find my street or look up my favourite building. For some people this seems to have reduced confidence in the data quality. Having only ever been half-way across the Brooklyn Bridge, instead I looked at the map to try and find interesting trends. Some are certainly visible, but I found myself wanting a map with some aggregation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What I tried.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So I made this. Yes, a post will follow on how.&lt;/p&gt;
&lt;p&gt;&lt;a href="/images/bk_r5_version4.png"&gt;&lt;img alt="bk_r5_version4" src="/images/bk_r5_version4-923x1024.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(click for the huge version)&lt;/p&gt;
&lt;p&gt;By showing the average build year for buildings in each hexagon, rather than plotting every single data point, some patterns are more visible. Though I will admit that once you've seen them in the above map, they are often quite visible in the original.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hexagon Size&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The size or "radius" of the hexagons is an important parameter of the visualization. As you can see clearly below, our ability to spot patterns of a certain size requires aggregation of a similar size. Compare the first with the last and you'll see.&lt;/p&gt;
&lt;p&gt;&lt;img alt="radius_5_400" src="/images/radius_5_400.png"&gt;&lt;img alt="radius_10_400" src="/images/radius_10_400.png"&gt;&lt;img alt="radius_20_400" src="/images/radius_20_400.png"&gt;&lt;img alt="radius_30_400" src="/images/radius_30_400.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aggregation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While my map is not a compelling improvement on the original, I think it is a nice demonstration of the power of aggregation. Data visualizers can be overly focused on plotting the data directly. Analysts, however, know that you can rarely accomplish anything without some good aggregation. This intersection of data viz and analysis is undeniably interesting.&lt;/p&gt;</content></entry><entry><title>Visualising on-line conversations: MOOC for-credit "discussion"</title><link href="/visualising-on-line-conversations-mooc-for-credit-discussion.html" rel="alternate"></link><published>2013-08-01T04:38:45+00:00</published><updated>2013-08-01T04:38:45+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-08-01:/visualising-on-line-conversations-mooc-for-credit-discussion.html</id><summary type="html">&lt;p&gt;Visualizing discussions is an interesting challenge. In this post I use a simple technique in the context of discussions in an on-line course.&lt;/p&gt;
&lt;p&gt;I have been auditing Dr. Anthony C. Robinson's Maps and the Geospatial Revolution on &lt;a href="https://www.coursera.org/"&gt;Coursera&lt;/a&gt;, a MOOC (Massive Open Online Course). 10% of a student's grade for …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Visualizing discussions is an interesting challenge. In this post I use a simple technique in the context of discussions in an on-line course.&lt;/p&gt;
&lt;p&gt;I have been auditing Dr. Anthony C. Robinson's Maps and the Geospatial Revolution on &lt;a href="https://www.coursera.org/"&gt;Coursera&lt;/a&gt;, a MOOC (Massive Open Online Course). 10% of a student's grade for the course comes from the weekly discussion assignments. If a student makes 20 posts within the 5 week course, they will receive full credit. When venturing into the forum for Week 1, I was struck by the low quality of the "discussion" taking place. It did not seem at all like an exchange of ideas.&lt;/p&gt;
&lt;p&gt;My gut told me that the participation credit was driving bad behavior. The data lover in me wanted to prove it with data. The data visualizer wants to see it to believe it.&lt;/p&gt;
&lt;p&gt;I found two threads to compare. In the general discussion forum I found a thread, "Where are you located?" and in the assignment forum I found "Where are you from?". More or less the same question (engaging discussions on the topic of identity aside), but with one key difference: the posts in the assignment forum were for credit. Take a look (click for bigger):&lt;/p&gt;
&lt;p&gt;&lt;a href="/images/where_r_u_publish1.png"&gt;&lt;img alt="where_r_u_publish" src="/images/where_r_u_publish1-1024x144.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Posts go in chronological order from left to right. Every author is a line. An author who posts only once is a dot. An author who posts more than once has his posts connected by a line.&lt;/p&gt;
&lt;p&gt;The difference is clear. While this does not constitute proof, it's certainly compelling.&lt;/p&gt;
&lt;p&gt;Statistics of note: The general discussion thread has 1.34 posts per author and 96 characters per post, compared to 1.20 posts per author and 81 characters per post in the discussion assignment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; I have been corrected. Apparently all posts in all forums count for credit, though this is not entirely clear from the syllabus. The key difference between the two posts remains, but must be restated: Participants in the assignment forum were there primarily for credit, while those in the general forum were there primarily to discuss.&lt;/p&gt;</content></entry><entry><title>My Ontario Sunshine List work in the Financial Post</title><link href="/my-ontario-sunshine-list-work-in-the-financial-post.html" rel="alternate"></link><published>2013-07-28T05:30:10+00:00</published><updated>2013-07-28T05:30:10+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-07-28:/my-ontario-sunshine-list-work-in-the-financial-post.html</id><summary type="html">&lt;p&gt;A little late posting this, but you can find my work cited in the Financial Post from July 13.&lt;/p&gt;
&lt;p&gt;Terence Corcoran writes &lt;a href="http://business.financialpost.com/opinion/terence-corcoran-the-sunshine-race-to-the-top"&gt;The sunshine race to the top&lt;/a&gt; and includes:&lt;/p&gt;
&lt;blockquote&gt;"A quick and dirty snapshot of trends was recently highlighted by statistical blogger Aleksey Nozdryn-Plotnicki. Following release of the 2013 …&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;A little late posting this, but you can find my work cited in the Financial Post from July 13.&lt;/p&gt;
&lt;p&gt;Terence Corcoran writes &lt;a href="http://business.financialpost.com/opinion/terence-corcoran-the-sunshine-race-to-the-top"&gt;The sunshine race to the top&lt;/a&gt; and includes:&lt;/p&gt;
&lt;blockquote&gt;"A quick and dirty snapshot of trends was recently highlighted by statistical blogger Aleksey Nozdryn-Plotnicki. Following release of the 2013 Ontario sunshine list, Mr. Nozdryn-Plotnicki’s posted a graph that showed big increases in the salaries of the province’s top 1000 income earners (see graph below). The highest paid individuals in the province recorded wage gains of 7.2% during 2012 “where everyone else saw 2.2%,” said Mr. Nozdryn-Plotnicki in a posting on his site, thinkdatavis.com. One year doesn’t make a trend, but similarly shaped graphs can be found for 2008, 2006, 2004 and 2000.

Another graphic example from thinkdatavis.com: Ontario’s pathologist salaries have increased 5.5% annually since 1996. During the first decade following the sunshine laws, pathologist packages increased 6.6% annually. That was followed by a four-year flat period. Then, in 2012, the pathologists recorded a single-year gain of 22%."&lt;/blockquote&gt;

&lt;p&gt;For those outside Canada, the Financial Post is a part of the National Post and &lt;a href="http://en.wikipedia.org/wiki/List_of_newspapers_in_Canada_by_circulation"&gt;according to Wikipedia&lt;/a&gt; had a weekly circulation of 800,000 in 2011.&lt;/p&gt;</content></entry><entry><title>The sun shines fondly on Ontario university presidents</title><link href="/the-sun-shines-fondly-on-ontario-university-presidents.html" rel="alternate"></link><published>2013-07-13T01:10:01+00:00</published><updated>2013-07-13T01:10:01+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-07-13:/the-sun-shines-fondly-on-ontario-university-presidents.html</id><summary type="html">&lt;p&gt;Since 1996, the ten best paid Ontario university presidents have seen their packages increased on average about 5.1% annually. This compares to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;4.0% for the 10,000th best paid person on the Sunshine List since 2001 (when there first were 10,000 names on the list)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3.5 …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Since 1996, the ten best paid Ontario university presidents have seen their packages increased on average about 5.1% annually. This compares to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;4.0% for the 10,000th best paid person on the Sunshine List since 2001 (when there first were 10,000 names on the list)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3.5% annually for the 20,000th best paid since 2004&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;5.5% annually for Pathologists since 1996&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While not as shocking as the single-year 22% raise that I &lt;a href="http://alekseynp.com/2013/04/19/20-25-raise-for-ontarios-pathologists-in-201-shows-sunshine-list/"&gt;previously identified for pathologists&lt;/a&gt;, this is evidence that university presidents are in the elite club for whom the sun shines strongly. As I have &lt;a href="http://alekseynp.com/2013/05/06/second-largest-growth-gap-in-2012-for-ontario-sunshine-list/"&gt;shown previously&lt;/a&gt;, those at the top in Ontario not only higher salaries, and bigger raises in absolute dollars, but also larger percentage increases annually which compound and lead to bigger gap between them and everyone else.&lt;/p&gt;
&lt;p&gt;Explore the data below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Packages for University Presidents in Ontario" src="/images/university-presidents.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Questions and Answers&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Why are there sudden dips in the lines?&lt;/p&gt;
&lt;p&gt;For example, Brock in 2007 and Carleton in 2006 and 2009. When an outgoing or incoming president works only a portion of a year, they earn less and are reported as less.&lt;/p&gt;
&lt;p&gt;Why is there missing years sometimes?&lt;/p&gt;
&lt;p&gt;As with sudden dips in the lines, if both the outgoing and incoming presidents work only a portion of the year and only earn less than $100,000, they will not qualify for disclosure and there will be no data for them. Also in some years the data may be outright missing.&lt;/p&gt;
&lt;p&gt;Why is there almost no data for Queen’s?&lt;/p&gt;
&lt;p&gt;I struggled to identify presidents of Queen’s in the data. Perhaps their structure is different. I took Daniel R. Woolf, Principal - Principal's Office for 2013, and Wade G. Hall, Executive Director Development for 2010 and 2009, but this is probably wrong. Any experts on the structure can feel free to inform me and I can make the appropriate correction.&lt;/p&gt;
&lt;p&gt;Why do many lines start after 1997?&lt;/p&gt;
&lt;p&gt;Before this period, the given presidents did not earn more than $100,000 and did not qualify for disclosure.&lt;/p&gt;</content></entry><entry><title>Data Sins of the Ontario Ministry of Finance – Public Sector Salary Disclosure</title><link href="/data-sins-of-the-ontario-ministry-of-finance-public-sector-salary-disclosure.html" rel="alternate"></link><published>2013-05-28T03:47:59+00:00</published><updated>2013-05-28T03:47:59+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-05-28:/data-sins-of-the-ontario-ministry-of-finance-public-sector-salary-disclosure.html</id><summary type="html">&lt;p&gt;Upfront warning: This article is likely to be only interesting to two types of people, those expecting to work with this data from the Ontario Ministry of Finance, and anyone in a position to demand better.&lt;/p&gt;
&lt;p&gt;Apart from their obvious failure to publish the data in clean, machine-readable formats like …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Upfront warning: This article is likely to be only interesting to two types of people, those expecting to work with this data from the Ontario Ministry of Finance, and anyone in a position to demand better.&lt;/p&gt;
&lt;p&gt;Apart from their obvious failure to publish the data in clean, machine-readable formats like csv or xls, the Ontario Ministry of Finance is making other important mistakes when making releases for the Public Salary Disclosure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Addenda&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The addenda is inconsistently applied to the master, published, available data. For each year an addenda is published, containing changes to the data from when it was published. There are three types of changes: additions, changes, and deletions. As far as I can tell, deletions are acted upon, and are reflected in the published set (this is so for 2012), but changes and additions are not. This is a big problem, because now no clear audit trail exists for the data, and people using the data from different times will disagree on basic facts like the number of employees disclosed.&lt;/p&gt;
&lt;p&gt;The inconsistent application of the addenda is also inconsistent. For instance, in 2009, the deletions have not be enacted upon the published data.&lt;/p&gt;
&lt;p&gt;Changes are listed with no reference to the original data, so what type of change is not known. In 2012 there were 328 records changed with one or more changes, 1 change of employer, 1 change of surname, 14 changes of given name, 175 changes of position, 135 changes of salary paid, 11 changes of taxable benefits. More disturbingly, 1 where the changed row matched exactly to the original row. Not a good indicator of good data discipline.&lt;/p&gt;
&lt;p&gt;In 2012, two supposed additions were in fact changes, one for position and one for employer name. This simple error will have consequences for anyone working, collaborating, or communicating with the data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inconsistent Pagination&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;2013 disclosure has 1,000 records per page across 104 pages&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2012 disclosure breaks up organisations (hospitals, colleges, etc.) in irregular chunks across 61 pages&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2011 disclosure keeps most organisations on a single page, with a few broken into only two pages for a total of 15 pages&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2010 similar to 2011 and 14 pages&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Inconsistent HTML Format&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;2013 and 2012 tables contain header information in thead and data in tbody&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2011 and before tables do not contain tbody, and simply have header information in the first tr&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Inconsistent Location for those seconded to ministries&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;2013 has it's own page&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Before it was a second table on the last page of ministries&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Other&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Inexplicably, a single record under schoolboards in 2007 has a taxable benefits of “101..92” rather than “&lt;span class="math"&gt;\(101.92” like the record immediately following it. Yes that's two decimals and no dollar sign. Inexplicably, a single record under other in 2007 has a salary paid of “\)&lt;/span&gt;115, 448.78”. Yes, there's a space after the comma. Yes, these may seem like small things, but they betray a deeper malpractice. A properly validated and typed data structure would not allow this abnormalities.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>Pathologists follow-up, data from 1996 to 2013 from Ontario "Sunshine List"</title><link href="/pathologists-follow-up-data-from-1996-to-2013-from-ontario-sunshine-list.html" rel="alternate"></link><published>2013-05-06T20:31:34+00:00</published><updated>2013-05-06T20:31:34+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-05-06:/pathologists-follow-up-data-from-1996-to-2013-from-ontario-sunshine-list.html</id><summary type="html">&lt;p&gt;This is a follow-up on my article, &lt;a href="../../../../2013/04/19/20-25-raise-for-ontarios-pathologists-in-201-shows-sunshine-list/"&gt;20-25% raise for Ontario’s pathologists in 2012&lt;/a&gt;. Evolving from an explorative analysis of the 2013 Ontario Public Sector Salary Disclosure, the "Sunshine List", I identified a surprising change in the packages (salary + benefits) of pathologists on the list.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further Analysis&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Extending the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is a follow-up on my article, &lt;a href="../../../../2013/04/19/20-25-raise-for-ontarios-pathologists-in-201-shows-sunshine-list/"&gt;20-25% raise for Ontario’s pathologists in 2012&lt;/a&gt;. Evolving from an explorative analysis of the 2013 Ontario Public Sector Salary Disclosure, the "Sunshine List", I identified a surprising change in the packages (salary + benefits) of pathologists on the list.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further Analysis&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Extending the analysis backwards to 1996, the start of the “Sunshine List” we can get a more complete picture of salary changes for Ontario pathologists.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Average Pathologist Salary By List Ranking" src="/images/pathologistsovertime.png"&gt;&lt;/p&gt;
&lt;p&gt;The analysis and the visual shows that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Since 1997, pathologist packages have increased on average 5.5% annually&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;From 1997 to 2007, packages increased 6.6% annually and exponentially which was clearly unsustainable&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;From 2007 to 2012, packages saw little to no growth&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In 2013, there was an unprecedented, single-year growth of 22%&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contact&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I contacted the Ontario Ministry of Health and Long-Term Care. Their media relations passed me on to the Ontario Association of Pathologists, whom I had already contacted and have yet to see a response. A sensible strategy for them would be to ignore enquiries from bloggers regardless of if they had anything to hide.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can read more on this topic in a&lt;a href="http://opinion.financialpost.com/2013/07/08/terence-corcoran-the-sunshine-race-to-the-top/"&gt; piece in the Financial Post&lt;/a&gt; which cites these findings.&lt;/p&gt;</content></entry><entry><title>Second largest growth-gap in 2012 for Ontario "Sunshine List"</title><link href="/second-largest-growth-gap-in-2012-for-ontario-sunshine-list.html" rel="alternate"></link><published>2013-05-06T12:00:38+00:00</published><updated>2013-05-06T12:00:38+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-05-06:/second-largest-growth-gap-in-2012-for-ontario-sunshine-list.html</id><summary type="html">&lt;p&gt;I have &lt;a href="http://alekseynp.com/7-2-raise-for-1000-best-paid-ontario-public-sector-employees/"&gt;previously shown&lt;/a&gt; that 2012 was a good year for the highest paid individuals in the Ontario Public Sector Salary Disclosure ("Sunshine List"). The top 1,000 best paid workers saw salary growth of average 7.2% where everyone else saw 2.2%. It can further be shown that …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have &lt;a href="http://alekseynp.com/7-2-raise-for-1000-best-paid-ontario-public-sector-employees/"&gt;previously shown&lt;/a&gt; that 2012 was a good year for the highest paid individuals in the Ontario Public Sector Salary Disclosure ("Sunshine List"). The top 1,000 best paid workers saw salary growth of average 7.2% where everyone else saw 2.2%. It can further be shown that 2012 was one of the biggest years for disproportionate growth at the top. Only 2008 shows a bigger gap between the 1,000 best paid and everyone else:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Salary Growth at Top of Sunshine List" src="/images/top4000_raise_2012.png"&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, We can see that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A similar shape to 2012 in 2008, 2006, 2004, and 2000. In all of these years, the very top (1000 or so) of the list saw considerably more growth than those near the top (ranks 1000 to 4000 or so).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2009-2011 were weak growth years, with 2009 and 2011 showing actually lower salary growth at the very top&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;More&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can read more on this topic in a&lt;a href="http://opinion.financialpost.com/2013/07/08/terence-corcoran-the-sunshine-race-to-the-top/"&gt; piece in the Financial Post&lt;/a&gt; which cites these findings.&lt;/p&gt;</content></entry><entry><title>Current publishing of Ontario "Sunshine List" not good enough</title><link href="/current-publishing-of-ontario-sunshine-list-not-good-enough.html" rel="alternate"></link><published>2013-04-22T12:00:26+00:00</published><updated>2013-04-22T12:00:26+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-04-22:/current-publishing-of-ontario-sunshine-list-not-good-enough.html</id><summary type="html">&lt;p&gt;Standing where we are in 2013, the Public Services Salary Disclosure Act of 1996 in Ontario seems ahead of it's time in terms of open government data. 17 consecutive lists published of all public sector employees who earned more than $100,000 in a year. But, what was once a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Standing where we are in 2013, the Public Services Salary Disclosure Act of 1996 in Ontario seems ahead of it's time in terms of open government data. 17 consecutive lists published of all public sector employees who earned more than $100,000 in a year. But, what was once a bold step forward in terms of public accountability, is now falling behind in other ways.&lt;/p&gt;
&lt;p&gt;By today's standards, publishing an intimidatingly long list of approaching 100,000 names and salaries across 100 or so HTML or PDF pages does not constitute disclosure. Sure, it's great if you want to look up how much your boss makes or to keep an eye on the salaries of TVO presenters, but after that it falters. Making data possible to access, and making it easy to access are different things. If the data were published in print, but not made available online, would that be acceptable? Was it in 1996?&lt;/p&gt;
&lt;p&gt;Any data journalist who wants to work with the data, must first scrape it from those hundred pages, which either requires some technical skill and some time, or brute force and quite a lot of time. Even answering simple questions like, “How many names are on the list?” and “What is the average salary?” have to wait for this scraping to be performed.&lt;/p&gt;
&lt;p&gt;What about the general public? Even if they've never heard of scraping a web page, they should still have natural questions like: How many people from each employer is on the list? How much money are CEOs making on average this year? Is that more than last year? How many people on the list are Pathologists?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Easy change&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At a minimum, the entire list should be made available for download in a single file in CSV and/or XLS format. This would remove barriers and save time for any data journalist wanting to access the information. This should be trivially easy to do, because by the looks of the URLs, the 2013 (for 2012) disclosure is already stored in a database.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Empower the ecosystem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Not only does this mean that citizens and journalists could better access the data, but it also would enable data visualisation and interaction practitioners to create tools for the entire public to access the information.&lt;/p&gt;</content></entry><entry><title>20-25% raise for Ontario's pathologists in 2012</title><link href="/20-25-raise-for-ontarios-pathologists-in-201-shows-sunshine-list.html" rel="alternate"></link><published>2013-04-19T16:58:10+00:00</published><updated>2013-04-19T16:58:10+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-04-19:/20-25-raise-for-ontarios-pathologists-in-201-shows-sunshine-list.html</id><summary type="html">&lt;p&gt;Evidence from the Ontario Public Sector Salary Disclosure, the so-called “Sunshine List”, shows that pathologists in Ontario saw an average salary increase in 2013 of 20-25% over the previous year. This average for the entire list was 2.2%.&lt;/p&gt;
&lt;p&gt;Appearing in both the 2013 and 2012 disclosures, 195 pathologists saw …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Evidence from the Ontario Public Sector Salary Disclosure, the so-called “Sunshine List”, shows that pathologists in Ontario saw an average salary increase in 2013 of 20-25% over the previous year. This average for the entire list was 2.2%.&lt;/p&gt;
&lt;p&gt;Appearing in both the 2013 and 2012 disclosures, 195 pathologists saw their average package (salary + taxable benefits) increase by $57k or 20.6% from $280k to $337k.  The top 200 earning pathologists in 2012 averaged $348k, a 25.4% increase over 2011.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ontario Public Salary Disclosure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Every year since 1996, the Ontario Ministry of Finance has released a list of all public sector employees who earned more than $100,000 in the previous year.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So what's happening here? Why are pathologists seeing a 25% raise while the rest of the list shows a very reasonable growth of 2.2%?&lt;/p&gt;
&lt;p&gt;At this point all we have are hypotheses. Analysis of the publicly available data has uncovered a surprising feature, and further investigation is required to find the cause. This is exactly the sort of process we should expect from an open government/open data initiative like the Sunshine List.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Detail&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The positions in the data mapped to pathologist were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Pathologist&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pathologist / Pathologiste&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pathologist Laboratory Medical Director and Chief of Medical Staff&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pathologist/Laboratory Medical Director&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pathologist/ Anatomopathologiste&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pathologist – Pathology / Pathologiste&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Associate Director Pathology / Directrice adjointe Pathologie&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Neuropathologist / Professor&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Neuropathologist&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Laboratory Pathologist / Pathologiste du laboratoire&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Laboratory Pathologist/Pathologiste du laboratoire&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Medical Director Clinical Lab Services / Pathologiste&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pathologist/Pathologiste&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Associate Director Pathology / Directeur adjoint Pathologie&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Associate Pathologist / Pathologiste adjoint&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Associate Pathologist/Pathologiste adjoint&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Senior Associate Pathologist&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Senior Associate Pathologist / Pathologiste associÃ©(e) principal(e)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Division Head Haematopathology&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Chief Pathology &amp;amp; Laboratory Director&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Associate Pathologist&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Associate Pathologist / Pathologiste associÃ©(e)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pathologist and Director Laboratory Medicine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pathologist/Director Laboratory Medicine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Director Pathology / Directeur Pathologie&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pathologist–in–Chief&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pathologist-in-Chief&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Associate Head of Pathology&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Associate Head Pathology&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Division Head Pathology&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pathologist – General / Pathologiste général&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pathologist - General/Pathologiste gÃ©nÃ©ral&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Anatomical Pathologist / Anatomopathologiste&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Anatomical Pathologist/Anatompoathologiste&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Chief Pathologist&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Administrative Director Pathology and Laboratory Medicine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Senior Pathologists Assistant&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Speech Pathologist – Voice&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Speech Pathologist Voice&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Administrative Director Pathology &amp;amp; Laboratory Medicine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Haematopathologist&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Senior Manager Pathology &amp;amp; Laboratory Medicine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Anatomic Pathologist / Professor&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pathologist &amp;amp; Discipline Director&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Sources:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.fin.gov.on.ca/en/publications/salarydisclosure/pssd/"&gt;http://www.fin.gov.on.ca/en/publications/salarydisclosure/pssd/&lt;/a&gt;&lt;/p&gt;</content></entry><entry><title>7.2% raise for 1,000 best paid Ontario public sector employees</title><link href="/7-2-raise-for-1000-best-paid-ontario-public-sector-employees.html" rel="alternate"></link><published>2013-04-19T16:40:12+00:00</published><updated>2013-04-19T16:40:12+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-04-19:/7-2-raise-for-1000-best-paid-ontario-public-sector-employees.html</id><summary type="html">&lt;p&gt;&lt;img alt="Ontario Sunshine List Salary Growth" src="/images/graphv2.png"&gt;]&lt;/p&gt;
&lt;p&gt;The top 1,000 employees with the highest package (salary + taxable benefits) in the Ontario Public Sector Salary Disclosure, the so-called “Sunshine List”, saw an average increase of almost $25,000 in 2012 compared to the previous year, an increase of 7.2%, much higher than the bottom half of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Ontario Sunshine List Salary Growth" src="/images/graphv2.png"&gt;]&lt;/p&gt;
&lt;p&gt;The top 1,000 employees with the highest package (salary + taxable benefits) in the Ontario Public Sector Salary Disclosure, the so-called “Sunshine List”, saw an average increase of almost $25,000 in 2012 compared to the previous year, an increase of 7.2%, much higher than the bottom half of the 80,000-strong list which saw an increase of only 2.2%.&lt;/p&gt;
&lt;p&gt;Is this cause for alarm? Highly paid CEO's are fully in the public spotlight, and the many many school principals have their pay closely monitored, but what about the highly paid individuals near, but not at the top? The data shows that for them, 2012 was a good year.&lt;/p&gt;
&lt;p&gt;Every year since 1996, the Ontario Ministry of Finance has released a list of all public sector employees who earned more than $100,000 in the previous year.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Oversight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can all see that “Sunshine List” champion Thomas Mitchell, President &amp;amp; CEO of Ontario Power Generation took a pay cut this year, but with approaching 100,000 names on the list, more sophisticated, data-drive oversight is possible.&lt;/p&gt;
&lt;p&gt;Government-friendly observes point out that the average salary on the list has decreased, just like last year, but that is a red herring. Anyone can add over 9,000 people earning just over $100k to a list with an average salary of $129k and bring down the average. As the list continues to grow from the bottom, we can expect the average salary to decline, without this being any indicator of public fiscal discipline.&lt;/p&gt;
&lt;p&gt;Opposition partisans will lament the increasing growth of the list, 9,600 more this year and 7,500 the year before. This is again misleading. The pyramid shape of any organisation tells us that there are more people as you move down the salary brackets. With a perfectly reasonable average salary growth at just over 2.5%, 9,600 employees graduated to the “Sunshine List” this year after having earned around $98k last year. Probably more than 9,600 employees, currently earning around $98k will be new additions to the list next year, and more the year after. Inflation and economic growth will ensure that the list grows, and the pyramid shape will ensure that it grows faster.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Top 1,000&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So who are these lucky 1,000 who on average made 7.2% more in 2012?&lt;/p&gt;
&lt;p&gt;This year the top 1000 best packages on the list included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;583 individuals working in hospitals&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;176 Pathologists&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;50 Chief Executive Officers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;66 Vice-Presidents (Senior, Executive, etc.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;79 Psychiatrists&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;86 employees in electricity&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;56 Vice-Presidents (Senior, Executive, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;144 working at Universities&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;100 Professors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Big raises&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Of the 1,000, 737 can be matched exactly by name and organisation type to last year. 92 of those fortunate souls saw an increase of over 25%! At the top of the pack was Mohamed Abelaziz Elbestawi, Vice-President Research/Professor at McMaster University who was reported as paid salary $266k in 2011 and $506k in 2012! Trung Kien Mai, a Pathologist at The Ottawa Hospital saw his paid salary move from $306k in 2011 to $515k in 2012!&lt;/p&gt;
&lt;p&gt;Of those 92 with big raises:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;83 work in hospitals&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;50 are Pathologists&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;More questions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At this point, this analysis raises more questions than it answers, but that is to be expected from an analysis of this salary disclosure data. The Public Salary Disclosure Act can help us find questions, not answers.&lt;/p&gt;
&lt;p&gt;What we do know is that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Salaries near the top grew substantially&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Those salaries grew much more, even on a % basis than those at the bottom&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Growth was higher than expected given slow economic growth&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some individuals can be shown to have experienced extraordinary raises&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pathologists do well, and 2012 was a particularly good year for some&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Source&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.fin.gov.on.ca/en/publications/salarydisclosure/pssd/"&gt;http://www.fin.gov.on.ca/en/publications/salarydisclosure/pssd/&lt;/a&gt;&lt;/p&gt;</content></entry><entry><title>Create: Information Tree</title><link href="/create-information-tree.html" rel="alternate"></link><published>2013-03-06T22:44:25+00:00</published><updated>2013-03-06T22:44:25+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-03-06:/create-information-tree.html</id><summary type="html">&lt;p&gt;Today I am releasing another tool that allows users to create and export a diagram/visualisation. Today it is an Information Tree.  &lt;/p&gt;
&lt;p&gt;&lt;a href="viz/create-visualisations/reingold-Tilford_Tree4/Reingold_Tilford_Tree.html"&gt;Click here&lt;/a&gt; to access the tool.&lt;/p&gt;
&lt;p&gt;&lt;a href="viz/create-visualisations/reingold-Tilford_Tree4/Reingold_Tilford_Tree.html"&gt;&lt;img alt="" src="viz/images/node_tree_blog.png"&gt;&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;Users can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Define a complete 4-level hierarchy, breaking a concept down to four levels&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Customise some aspects&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Export to SVG&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today I am releasing another tool that allows users to create and export a diagram/visualisation. Today it is an Information Tree.  &lt;/p&gt;
&lt;p&gt;&lt;a href="viz/create-visualisations/reingold-Tilford_Tree4/Reingold_Tilford_Tree.html"&gt;Click here&lt;/a&gt; to access the tool.&lt;/p&gt;
&lt;p&gt;&lt;a href="viz/create-visualisations/reingold-Tilford_Tree4/Reingold_Tilford_Tree.html"&gt;&lt;img alt="" src="viz/images/node_tree_blog.png"&gt;&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;Users can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Define a complete 4-level hierarchy, breaking a concept down to four levels&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Customise some aspects&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Export to SVG&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is an alpha release of the tool, hopefully the first of many. Any and all feedback is welcome.&lt;/p&gt;
&lt;p&gt;Also, if you have the time and ability to do similar or better things, I invite you to contact me regarding collaboration.&lt;/p&gt;</content></entry><entry><title>Create: Information Wheel</title><link href="/create-information-wheel.html" rel="alternate"></link><published>2013-02-25T19:20:28+00:00</published><updated>2013-02-25T19:20:28+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2013-02-25:/create-information-wheel.html</id><summary type="html">&lt;p&gt;Today I am releasing a tool that allows users to create and export an Information Wheel.  &lt;/p&gt;
&lt;p&gt;&lt;a href="viz/create-visualisations/infowheel4/infoWheel.html"&gt;Click here&lt;/a&gt; to access the tool.&lt;/p&gt;
&lt;p&gt;&lt;a href="viz/create-visualisations/infowheel4/infoWheel.html"&gt;&lt;img alt="" src="viz/images/infowheel_blog.png"&gt;&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;Users can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Define a complete 4-level hierarchy, breaking a concept down to four levels&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Customise ring-sizes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Customise colouring&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Customsie text&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Export to SVG&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is an alpha release …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today I am releasing a tool that allows users to create and export an Information Wheel.  &lt;/p&gt;
&lt;p&gt;&lt;a href="viz/create-visualisations/infowheel4/infoWheel.html"&gt;Click here&lt;/a&gt; to access the tool.&lt;/p&gt;
&lt;p&gt;&lt;a href="viz/create-visualisations/infowheel4/infoWheel.html"&gt;&lt;img alt="" src="viz/images/infowheel_blog.png"&gt;&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;Users can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Define a complete 4-level hierarchy, breaking a concept down to four levels&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Customise ring-sizes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Customise colouring&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Customsie text&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Export to SVG&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is an alpha release of the tool, hopefully the first of many. Any and all feedback is welcome.&lt;/p&gt;
&lt;p&gt;Also, if you have the time and ability to do similar or better things, I invite you to contact me regarding collaboration.&lt;/p&gt;</content></entry><entry><title>New Project: K-means Clustering</title><link href="/new-project-k-means-clustering.html" rel="alternate"></link><published>2012-12-18T17:00:35+00:00</published><updated>2012-12-18T17:00:35+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2012-12-18:/new-project-k-means-clustering.html</id><summary type="html">&lt;p&gt;When it comes to data visualisation design, it's always important to consider your purpose and your audience. Are you trying to convince your audience of a particular point of view? Are you giving your audience an platform from which to explore and find their own insights? In my latest piece …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When it comes to data visualisation design, it's always important to consider your purpose and your audience. Are you trying to convince your audience of a particular point of view? Are you giving your audience an platform from which to explore and find their own insights? In my latest piece I take a step down a less discussed path.&lt;/p&gt;
&lt;p&gt;I have created an interactive tool using D3.js that gives the user a chance to see and interact with the typical k-means clustering algorithm from data mining/machine learning. It is my hope, that it will enable students to develop an intuition for how the algorithm works, and a better appreciation of its shortcomings.&lt;/p&gt;
&lt;p&gt;You can learn more about k-means clustering &lt;a href="http://en.wikipedia.org/wiki/K-means_clustering"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="viz/k-means.html"&gt;K-means Clustering&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;&lt;a href="viz/k-means.html"&gt;&lt;img alt="" src="/images/kmeans.png"&gt;&lt;/a&gt;&lt;/p&gt;</content></entry><entry><title>New Project: Paid to Win</title><link href="/new-project-paid-to-win.html" rel="alternate"></link><published>2012-12-09T16:51:41+00:00</published><updated>2012-12-09T16:51:41+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2012-12-09:/new-project-paid-to-win.html</id><summary type="html">&lt;p&gt;Here's one I had ready to go four weeks ago, but the unreliability of my current web provider got the better of me. This is another piece working from the BBC Price of Football Survey data, only this time mashing it up with league tables available from Wikipedia. I ask …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here's one I had ready to go four weeks ago, but the unreliability of my current web provider got the better of me. This is another piece working from the BBC Price of Football Survey data, only this time mashing it up with league tables available from Wikipedia. I ask where you find the cheapest goals and cheapest wins throughout the English and Scottish Football leagues:  &lt;/p&gt;
&lt;p&gt;&lt;a href="viz/paid-to-win.html"&gt;&lt;strong&gt;Paid to Win&lt;/strong&gt;&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;&lt;a href="viz/paid-to-win.html"&gt;&lt;img alt="" src="/images/paidtowin.png"&gt;&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;Click through for the interactive version.  &lt;/p&gt;
&lt;p&gt;This is again quite similar to the &lt;a href="viz/english-football-tickets-value-for-money.html"&gt;English Football Value for Money&lt;/a&gt; piece I did previously and is mainly just a data remix with an extra dimension to filter on. It's an interesting way to explore a ranking against two parameters at the same time.  &lt;/p&gt;
&lt;p&gt;Advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Display a ranking against two parameters simultaneously&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Display both as a ranking as well as relative values. You can easily see both what is higher and lower and also by how much&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Easily compare two teams against both parameters as well as their relative performance against both parameters (i.e. comparing line slopes)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Easily identify big movers between the two measures&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disadvantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Slightly complex, so requires the reader to figure it out a bit&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Alternative to showing two rankings at once would be to take a design decision to only show one, that which is deemed most important&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Current solution requires hover which is not mobile-friendly&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solution is D3.js and therefore SVG and thus IE 7- unfriendly&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>Guest Blog @ Junk Charts: Popcorn infographics</title><link href="/guest-blog-junk-charts-popcorn-infographics.html" rel="alternate"></link><published>2012-11-09T14:20:43+00:00</published><updated>2012-11-09T14:20:43+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2012-11-09:/guest-blog-junk-charts-popcorn-infographics.html</id><summary type="html">&lt;p&gt;Check out my guest entry at &lt;a href="http://junkcharts.typepad.com/junk_charts/"&gt;Junk Charts&lt;/a&gt;: &lt;a href="http://junkcharts.typepad.com/junk_charts/2012/11/guest-blog-popcorn-infographics.html"&gt;http://junkcharts.typepad.com/junk_charts/2012/11/guest-blog-popcorn-infographics.html&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Check out my guest entry at &lt;a href="http://junkcharts.typepad.com/junk_charts/"&gt;Junk Charts&lt;/a&gt;: &lt;a href="http://junkcharts.typepad.com/junk_charts/2012/11/guest-blog-popcorn-infographics.html"&gt;http://junkcharts.typepad.com/junk_charts/2012/11/guest-blog-popcorn-infographics.html&lt;/a&gt;&lt;/p&gt;</content></entry><entry><title>New Project: English Football Value for Money</title><link href="/new-project-english-football-value-for-money.html" rel="alternate"></link><published>2012-11-04T22:34:06+00:00</published><updated>2012-11-04T22:34:06+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2012-11-04:/new-project-english-football-value-for-money.html</id><summary type="html">&lt;p&gt;The Guardian Data Blog has made available for download both the BBC Price of Football Survey data and the Premier League Accounts data, and I thought it would be interesting to mash them up and look for value for money on tickets. I did the analysis and produced the following …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The Guardian Data Blog has made available for download both the BBC Price of Football Survey data and the Premier League Accounts data, and I thought it would be interesting to mash them up and look for value for money on tickets. I did the analysis and produced the following:&lt;/p&gt;
&lt;p&gt;&lt;a href="viz/english-football-tickets-value-for-money.html"&gt;&lt;strong&gt;English Footbal Tickets: Value For Money&lt;/strong&gt;&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;&lt;a href="viz/english-football-tickets-value-for-money.html"&gt;&lt;img alt="" src="/images/footievalue.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Click through for the interactive version.&lt;/p&gt;
&lt;p&gt;This piece straightforwardly shows both the rankings and the values for the two different ticket types across clubs. By hovering over a club name, or any line, you can see all the data relating to that club, tying the two nicely together. Unfortunately hover is not really available on mobile, so this is not a nice universal solution. Ironically mobile platforms are less interactive in some ways.&lt;/p&gt;
&lt;p&gt;This is also my cleanest application of d3.js to date, as I become more comfortable with the library.&lt;/p&gt;</content></entry><entry><title>New Project: Passing Direction of Premier League Football Players</title><link href="/new-project-passing-direction-of-premier-league-football-players.html" rel="alternate"></link><published>2012-10-20T12:33:36+00:00</published><updated>2012-10-20T12:33:36+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2012-10-20:/new-project-passing-direction-of-premier-league-football-players.html</id><summary type="html">&lt;p&gt;The guys over at &lt;a href="http://www.mcfc.co.uk/the-club/mcfc-analytics"&gt;MCFC Analytics&lt;/a&gt; have released a dataset for the entire 2010-11 English Premier League Football season. This has generated a number of visuals on passing. For the time being, only aggregate by-game data is available for the entire season, but in-game event data for all games should …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The guys over at &lt;a href="http://www.mcfc.co.uk/the-club/mcfc-analytics"&gt;MCFC Analytics&lt;/a&gt; have released a dataset for the entire 2010-11 English Premier League Football season. This has generated a number of visuals on passing. For the time being, only aggregate by-game data is available for the entire season, but in-game event data for all games should follow. This is my first project, likely in a series on MCFC data.&lt;/p&gt;
&lt;p&gt;&lt;a href="viz/mcfc-opta-passing-permier-league-2011-12.html"&gt;&lt;strong&gt;Passing Direction of Premier League Football Players&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="viz/mcfc-opta-passing-permier-league-2011-12.html"&gt;&lt;img alt="" src="/images/passingDirection1.png"&gt;&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;This visualisation is an excellent tool for &lt;strong&gt;outlier identification&lt;/strong&gt;. With players scattered and coloured by position, we can immediately see who the oddballs are. Who are the defenders that pass more like a midfielder? Vise versa? If I were more intimately familiar with the game, this would lead me down a path of investigation to determine what is different about that player. Is it interesting? Is it positive?&lt;/p&gt;
&lt;p&gt;Similar analyses carry value in the business world also. A retailer may have many locations and formats, but within those formats are there stores that behave outside of the norm? Do we have large format stores that look more like small format stores in the data? Is there a cost savings opportunity?&lt;/p&gt;
&lt;p&gt;In terms of technology, this is another application of &lt;a href="http://d3js.org/"&gt;D3.js&lt;/a&gt; and SVG. Unfortunately that means it won't work on browsers IE8 and below. If this piece were to be part of a wide-ranging, consumer-facing project, then a graceful degradation for IE8 would be required. If this were an internal business tool, a particular browser could be mandated (though this unfortunately might be IE8 or below). Since this is a personal project, I choose not to spend the hours to support IE8.&lt;/p&gt;</content></entry><entry><title>Global Games, Regional Sports - Vancouver 2010</title><link href="/global-games-regional-sports-vancouver-2010.html" rel="alternate"></link><published>2012-08-31T19:00:36+00:00</published><updated>2012-08-31T19:00:36+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2012-08-31:/global-games-regional-sports-vancouver-2010.html</id><summary type="html">&lt;p&gt;I &lt;a href="new-project-global-games-regional-sports-london-2012/"&gt;previously posted&lt;/a&gt; a project I worked on for the London 2012 Olympics. Well, I did some digging around and I managed to find data for the Vancouver 2010 Winter Olympics, so I wrangled it into the format I had previously used, made some tweaks, and republished my Global Games …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I &lt;a href="new-project-global-games-regional-sports-london-2012/"&gt;previously posted&lt;/a&gt; a project I worked on for the London 2012 Olympics. Well, I did some digging around and I managed to find data for the Vancouver 2010 Winter Olympics, so I wrangled it into the format I had previously used, made some tweaks, and republished my Global Games, Regional Sports analysis, but this time with different data. Does that make this some kind of data-remix?&lt;/p&gt;
&lt;p&gt;Please find the result &lt;a href="viz/global-games-regional-sports-vancouver-2010-olympics.html"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="viz/global-games-regional-sports-vancouver-2010-olympics.html"&gt;&lt;img alt="" src="/images/screenshot1.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample Insights&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Biathlon and Cross Country Skiing both move together towards Northern Europe when switching from participants to medals. Due to similarities in the sports, we would expect them to be linked.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Skeleton is most southern in participants due largely to Kiwi competitors, but as they don't win medals it rockets up to the top when switching to medals.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Four of six ice hockey medals won by Canada and the United States move that sport far to the west. A bad Olympics for European Ice Hockey&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Vancouver 2010 Medals by Athlete - &lt;a href="http://winterolympics.external.bbc.co.uk/medals/2010-standings/athletes/index.html"&gt;http://winterolympics.external.bbc.co.uk/medals/2010-standings/athletes/index.html&lt;/a&gt;
Participants from Contextures Blog via &lt;a href="http://blog.contextures.com/archives/2010/02/19/excel-pivot-tables-at-the-olympics/"&gt;http://blog.contextures.com/archives/2010/02/19/excel-pivot-tables-at-the-olympics/&lt;/a&gt;, thank you for manually scraping, compiling, and publishing
Latitude and Longitude of Capitals - Mix of sources, primarily Wikipedia&lt;/p&gt;</content></entry><entry><title>New Project: Who Won the Olympics?</title><link href="/new-project-who-won-the-olympics.html" rel="alternate"></link><published>2012-08-20T17:35:36+00:00</published><updated>2012-08-20T17:35:36+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2012-08-20:/new-project-who-won-the-olympics.html</id><summary type="html">&lt;p&gt;The 2012 London Summer Olympics are over and the medals are tallied. As it is probably the most nationalistic event of the year, we all look to the medal tables to see how our nation did. So, "Who won the Olympics?". Like any good question, the answer is: It depends …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The 2012 London Summer Olympics are over and the medals are tallied. As it is probably the most nationalistic event of the year, we all look to the medal tables to see how our nation did. So, "Who won the Olympics?". Like any good question, the answer is: It depends.&lt;/p&gt;
&lt;p&gt;I've created this visualisation to explore that question: &lt;img alt="" src="viz/who-won-the-olympics.html"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://alekseynp.com/portfolio/who-won-the-olympics.html"&gt;&lt;img alt="" src="/images/screenshot.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It's not as well designed and doesn't have the affordance that I would like, but I'm publishing today in its draft form.&lt;/p&gt;
&lt;p&gt;Guidance for usage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The 2x2 space is created by valuing Golds between 1 and 10 Silvers and Bronzes between 0 and 1 Silvers. Depending on where you are in the space, the rankings are different.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click a country in the rankings table on the right to see their information and thresholds&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click a region in the chart to see the ranking that corresponds to that combination of Gold and Bronze valuation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cross a red line to turn it blue and increase the ranking of your selected country. Conversely, cross a blue line to turn it red and decrease the ranking.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When hovering you can also see the rank for your selected country if you were to click&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When hovering, the darker shaded regions are near to your current in terms of rank for your selected country, the lighter are further.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Clearly the United States came top with more golds, more silvers and more medals than anybody else. China is similarly a sure bet for second. In the standard medals table countries are ranked by number of golds with other medals used to break ties. This is convenient for the host nation, Great Britain, who scored lot's of golds and inconvenient for Russia who didn't.&lt;/p&gt;
&lt;p&gt;So what about those Silvers and Bronzes? Are they worth something? Is a Gold worth three Silvers? Is a Bronze worth half a Silver? These two parameters, the value of a Gold and the value of a Bronze, enable us to create a two-dimensional space to explore. What are the possible different rankings for a given nation? What ranking corresponds to my valuation of medals?&lt;/p&gt;
&lt;p&gt;Interesting insights:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For Great Britain to come 3rd, they require gold medals to be worth at least nearly 2 silvers, and if bronzes are worth something, golds may need to be worth as much as 4.5 silvers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Canada is probably most vulnerable to changes in values compared to any other country. With a huge bronze haul and not much else, Canada could rank as low as 36th or as high as 13th, where either gold medals are nearly everything (36th) or all medals are created equal (13th)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For many countries, an increasing valuation of golds or bronzes is strictly a good thing, but for some, it actually gets more complex. Iran, for example: high valuations of gold and bronze are generally a bad thing for Iran, except when the value of gold is very high&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>New Project: Global Games, Regional Sports - London 2012</title><link href="/new-project-global-games-regional-sports-london-2012.html" rel="alternate"></link><published>2012-08-14T15:20:37+00:00</published><updated>2012-08-14T15:20:37+00:00</updated><author><name>Aleksey Nozdryn-Plotnicki</name></author><id>tag:None,2012-08-14:/new-project-global-games-regional-sports-london-2012.html</id><summary type="html">&lt;p&gt;Today I am publishing a project that I have been working on over the last couple of weeks during the 2012 Summer Olympics here in London. The essential question I asked was: If you calculate the weighted geographical midpoint of each sport at the Olympics, do you get an interesting …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today I am publishing a project that I have been working on over the last couple of weeks during the 2012 Summer Olympics here in London. The essential question I asked was: If you calculate the weighted geographical midpoint of each sport at the Olympics, do you get an interesting result? I think the answer is yes, and I built a visualisation to support exploring it.&lt;/p&gt;
&lt;p&gt;Please find the visualisation here: &lt;img alt="" src="viz/global-games-regional-sports-london-2012-olympics.html"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://alekseynp.com/portfolio/global-games-regional-sports-london-2012-olympics.html"&gt;&lt;img alt="" src="/images/screenshot2.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample Insights&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Any east/west or north/south participation bias is emphasised in the medal results. For example, a sport that has more eastern participants will have proportionally even more eastern medals. Examples: Badminton, Table Tennis, Archery. There are a few exceptions including Boxing with southern participants and northern winners.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Athletics have an incredibly wide range of participants and much narrower set of winners, dominated by the United States, pulling the medals point to the West&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Football and Hockey are quite southern sports. At least in the case of Football this is a result of low European participation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Very few sports have proportionally more southern winners than participants. Of the 32 sports, only six move south when switching from participants to medals: Beach Volleyball, Modern Pentathlon, Rowing, Sailing, Volleyball, Water Polo. This is due in part to the dominance of the United States and China in the games, but is true more generally as more northern countries tend to lead the medals table.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A lot of sports land where my intuition puts them: Northwestern European tennis, European handball, New world Beach Volleyball&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some surprises to me: Judo and Taekwondo are surprisingly central. Fencing appears European as well as east-Asian. Surprisingly international: Basketball. Probably not all surprises for all people.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2012 Olympic Medals by Country by Sport - &lt;a href="http://www.london2012.com/medals/medal-count/"&gt;http://www.london2012.com/medals/medal-count/&lt;/a&gt;, scraped manually.
All London 2012 athletes and medal data - &lt;a href="https://docs.google.com/spreadsheet/ccc?key=0AuKpKzUJbSqtdEdDR29BY0JsRDFlbHQ1SVRHcjlsLWc"&gt;https://docs.google.com/spreadsheet/ccc?key=0AuKpKzUJbSqtdEdDR29BY0JsRDFlbHQ1SVRHcjlsLWc&lt;/a&gt; for participants data, thank you The Guardian
Latitude and Longitude of Capitals - Mix of sources, primarily Wikipedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Technology&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This visualisation uses D3.js (&lt;a href="http://d3js.org/"&gt;http://d3js.org/&lt;/a&gt;), a JavaScript framework for SVG. Note that this then requires Firefox, Chrome, Safari, Opera, or IE9. Note that IE8 is explicitly excluded from this list.&lt;/p&gt;</content></entry></feed>